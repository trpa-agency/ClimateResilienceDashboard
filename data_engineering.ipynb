{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reduce Tahoe region greenhouse gas (GHG) emissions and support the measurement of carbon sequestration.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GHG Data\n",
    "ghgTable = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/124\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Track poor air quality, wildfire smoke, and extreme heat trends regionally.\n",
    "* https://dev.meteostat.net/python/\n",
    "* https://climatecheck.com/california/sacramento#:~:text=The%20number%20of%20the%20hottest,per%20year%20over%20102.3%C2%BAF.\n",
    "* https://dev.meteostat.net/python/api/point/#parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme heat days data from https://dev.meteostat.net/python/\n",
    "# https://climatecheck.com/california/sacramento#:~:text=The%20number%20of%20the%20hottest,per%20year%20over%20102.3%C2%BAF.\n",
    "# https://dev.meteostat.net/python/api/point/#parameters\n",
    "\n",
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Set time period\n",
    "start = datetime(2003, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "# Create Point for Lake Tahoe\n",
    "tahoe = Point(39.0001, -120.0001, 70)\n",
    "\n",
    "# adjust attributes fro tahoe\n",
    "tahoe.radius = 2000000\n",
    "tahoe.method = 'weighted'\n",
    "\n",
    "# Get daily data for 2018\n",
    "data = Daily(tahoe, start, end)\n",
    "data = data.fetch()\n",
    "\n",
    "# convert all fields to farhenheit\n",
    "data = data.assign(MaxTemp = lambda x: (9/5)*x['tmax']+32)\n",
    "data = data.assign(MinTemp = lambda x: (9/5)*x['tmin']+32)\n",
    "data = data.assign(AvgTemp = lambda x: (9/5)*x['tavg']+32)\n",
    "\n",
    "# Plot daily temperature data\n",
    "fig = px.scatter(data, x=data.index, y=['AvgTemp', 'MinTemp', 'MaxTemp'], title='Daily Temperature in Tahoe')\n",
    "\n",
    "# add trendline\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.add_scatter(x=data.index, y=data['AvgTemp'].rolling(window=30).mean(), mode='lines', name='30 Day Avg')\n",
    "fig.update_layout(\n",
    "    title=\"Daily Temperature in Tahoe\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Temperature (F)\",\n",
    "    legend_title=\"Temperature\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html('html/1.2(a)_TahoeTemp.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme precip days\n",
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Set time period\n",
    "start = datetime(2003, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "# Create Point for Lake Tahoe\n",
    "tahoe = Point(39.0001, -120.0001, 70)\n",
    "\n",
    "# adjust attributes fro tahoe\n",
    "tahoe.radius = 2000000\n",
    "tahoe.method = 'weighted'\n",
    "\n",
    "# Get daily data for 2018\n",
    "data = Daily(tahoe, start, end)\n",
    "data = data.fetch()\n",
    "\n",
    "# convert all fields to farhenheit\n",
    "data = data.assign(MaxTemp = lambda x: (9/5)*x['tmax']+32)\n",
    "data = data.assign(MinTemp = lambda x: (9/5)*x['tmin']+32)\n",
    "data = data.assign(AvgTemp = lambda x: (9/5)*x['tavg']+32)\n",
    "\n",
    "# Plot daily temperature data\n",
    "fig = px.scatter(data, x=data.index, y=['prcp'], title='Daily Precipitation in Tahoe')\n",
    "\n",
    "# add trendline\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.add_scatter(x=data.index, y=data['prcp'].rolling(window=30).mean(), mode='lines', name='30 Day Avg')\n",
    "fig.update_layout(\n",
    "    title=\"Daily Temperature in Tahoe\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Precipitation (mm)\",\n",
    "    legend_title=\"Precipitation\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html('html/1.2(a)_TahoeTemp.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert prcp mm to inches\n",
    "data = data.assign(PRECIP = lambda x: x['prcp']*0.0393701)\n",
    "# create a dataframe with the number of extreme heat days\n",
    "extremePrecipDays = data[data['PRECIP'] > 1]\n",
    "\n",
    "extremePrecipDays = extremePrecipDays.assign(Count = 1)\n",
    "extremePrecipDays = extremePrecipDays.resample('Y').sum()\n",
    "extremePrecipDays = extremePrecipDays.assign(Year = extremePrecipDays.index.year)\n",
    "\n",
    "# plot the number of extreme heat days\n",
    "fig = px.bar(extremePrecipDays, x='Year', y='Count', title='Number of Extreme Precipitation Days in Tahoe (over 1\")')\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Number of Days\",\n",
    "    legend_title=\"Precipitation\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html('html/1.2(a)_ExtremeHeatDays.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of days over 90 degrees\n",
    "extremeHeatDays = data[data['MaxTemp'] > 90].shape[0]\n",
    "print(extremeHeatDays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the number of extreme heat days\n",
    "extremeHeatDaysDF = data[data['MaxTemp'] > 85]\n",
    "extremeHeatDaysDF = extremeHeatDaysDF.assign(Count = 1)\n",
    "extremeHeatDaysDF = extremeHeatDaysDF.resample('Y').sum()\n",
    "extremeHeatDaysDF = extremeHeatDaysDF.assign(Year = extremeHeatDaysDF.index.year)\n",
    "\n",
    "# plot the number of extreme heat days\n",
    "fig = px.bar(extremeHeatDaysDF, x='Year', y='Count', title='Number of Extreme Heat Days in Tahoe (over 85 degrees F)')\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Number of Days\",\n",
    "    legend_title=\"Temperature\"\n",
    ")\n",
    "# fig.write_html('html/1.2(a)_ExtremeHeatDays.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme heat days Reno\n",
    "reno = Point(39.5296, -119.8138, 1340)\n",
    "reno.radius = 2000000\n",
    "reno.method = 'weighted'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme heat days Sacramento\n",
    "sacramento = Point(38.575764, -121.478851, 6)\n",
    "sacramento.radius = 2000000\n",
    "sacramento.method = 'weighted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extreme heat days data from https://dev.meteostat.net/python/\n",
    "# https://climatecheck.com/california/sacramento#:~:text=The%20number%20of%20the%20hottest,per%20year%20over%20102.3%C2%BAF.\n",
    "# https://dev.meteostat.net/python/api/point/#parameters\n",
    "\n",
    "# Import Meteostat library and dependencies\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "from meteostat import Point, Daily\n",
    "\n",
    "# Set time period\n",
    "start = datetime(1083, 1, 1)\n",
    "end = datetime(2023, 12, 31)\n",
    "\n",
    "# # Create Point for Lake Tahoe\n",
    "# tahoe = Point(39.0001, -120.0001, 70)\n",
    "# tahoe.radius = 2000000\n",
    "# tahoe.method = 'weighted'\n",
    "# extreme heat days Sacramento\n",
    "sacramento = Point(38.575764, -121.478851, 6)\n",
    "sacramento.radius = 20000\n",
    "sacramento.method = 'weighted'\n",
    "\n",
    "# Get daily data for 2018\n",
    "data = Daily(sacramento, start, end)\n",
    "data = data.fetch()\n",
    "\n",
    "# convert all fields to farhenheit\n",
    "data = data.assign(MaxTemp = lambda x: (9/5)*x['tmax']+32)\n",
    "data = data.assign(MinTemp = lambda x: (9/5)*x['tmin']+32)\n",
    "data = data.assign(AvgTemp = lambda x: (9/5)*x['tavg']+32)\n",
    "\n",
    "# Plot daily temperature data\n",
    "fig = px.scatter(data, x=data.index, y=['AvgTemp', 'MinTemp', 'MaxTemp'], title='Daily Temperature in Tahoe')\n",
    "\n",
    "# add trendline\n",
    "fig.update_traces(mode='lines+markers')\n",
    "fig.add_scatter(x=data.index, y=data['AvgTemp'].rolling(window=30).mean(), mode='lines', name='30 Day Avg')\n",
    "fig.update_layout(\n",
    "    title=\"Daily Temperature in Tahoe\",\n",
    "    xaxis_title=\"Date\",\n",
    "    yaxis_title=\"Temperature (F)\",\n",
    "    legend_title=\"Temperature\"\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# fig.write_html('html/1.2(a)_TahoeTemp.html')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dataframe with the number of extreme heat days\n",
    "extremeHeatDaysDF = data[data['MaxTemp'] > 105]\n",
    "extremeHeatDaysDF = extremeHeatDaysDF.assign(Count = 1)\n",
    "extremeHeatDaysDF = extremeHeatDaysDF.resample('Y').sum()\n",
    "extremeHeatDaysDF = extremeHeatDaysDF.assign(Year = extremeHeatDaysDF.index.year)\n",
    "\n",
    "# plot the number of extreme heat days\n",
    "fig = px.bar(extremeHeatDaysDF, x='Year', y='Count', title='Number of Extreme Heat Days in Sacramento (over 100 degrees F)')\n",
    "fig.update_layout(\n",
    "    xaxis_title=\"Year\",\n",
    "    yaxis_title=\"Number of Days\",\n",
    "    legend_title=\"Temperature\"\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## getting Purple Air data\n",
    "\n",
    "# TRPA Air Quality Monitoring data\n",
    "trpaAirqualityStationLayer = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/16\"\n",
    "trpaairqualityDailyTable   = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/17\"\n",
    "trpaairqualityYearlyTable  = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/46\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "from arcgis.features import FeatureLayer\n",
    "import pandas as pd\n",
    "import pytz\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Gets data from the TRPA server\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    # return data frame\n",
    "    return all_data\n",
    "\n",
    "# Function to convert Unix timestamp to UTC datetime\n",
    "def convert_to_utc(timestamp):\n",
    "    return datetime.utcfromtimestamp(timestamp // 1000).replace(tzinfo=pytz.utc)\n",
    "\n",
    "# get fire start date as a table\n",
    "fireStartTable = \"https://services1.arcgis.com/jUJYIo9tSA7EHvfZ/arcgis/rest/services/California_Fire_Perimeters/FeatureServer/0\"\n",
    "# Purple Air data\n",
    "purpleAirURL = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/143\"\n",
    "\n",
    "dfPurpleAir = get_fs_data(purpleAirURL)\n",
    "dfFire = get_fs_data(fireStartTable)\n",
    "\n",
    "\n",
    "# remove nulls from ALARM_DATE\n",
    "dfFire = dfFire.dropna(subset=['ALARM_DATE'])\n",
    "# remove negative numbers in ALARM_DATE\n",
    "dfFire = dfFire[dfFire['ALARM_DATE'] > 0]\n",
    "\n",
    "# groupby date and get the mean of pm25\n",
    "dfAir = dfPurpleAir.groupby('date')['mean_pm25'].mean().reset_index()\n",
    "\n",
    "# Apply the function to the column\n",
    "dfFire['Date'] = dfFire['ALARM_DATE'].apply(convert_to_utc)\n",
    "\n",
    "# convert Date to month day year\n",
    "dfFire['Date'] = dfFire['Date'].dt.strftime('%m/%d/%Y')\n",
    " \n",
    "# convert date to datetime\n",
    "dfFire['Date'] = pd.to_datetime(dfFire['Date'], format='%m/%d/%Y')\n",
    "\n",
    "# filter date to 2018 or later\n",
    "dfFire = dfFire[dfFire['Date'] >= '01/01/2018']\n",
    "\n",
    "# filter FIRE_NAME is ['CALDOR', 'TAMARACK', 'FERGUSON', 'MOSQUITO', 'LOYALTON']\n",
    "dfFire = dfFire.loc[dfFire['FIRE_NAME'].isin(['CALDOR', 'TAMARACK', 'FERGUSON', 'MOSQUITO', 'LOYALTON'])]\n",
    "\n",
    "# only keep FIRE_NAME, GIS_ACRES, and Date\n",
    "dfFire = dfFire[['FIRE_NAME', 'GIS_ACRES', 'Date']]\n",
    "\n",
    "# rename columns\n",
    "dfFire = dfFire.rename(columns={'FIRE_NAME': 'Fire', 'GIS_ACRES': 'Acres'})\n",
    "\n",
    "# change column name to Date and PM 2.5 (ug/m3)\n",
    "dfAir = dfAir.rename(columns={'date': 'Date', 'mean_pm25': 'PM 2.5 (ug/m3)'})\n",
    "\n",
    "# Apply the function to the column\n",
    "dfAir['Date'] = dfAir['Date'].apply(convert_to_utc)\n",
    "# # convert Date to str\n",
    "dfAir['Date'] = dfAir['Date'].dt.strftime('%m/%d/%Y')\n",
    "# # convert Date to datetime\n",
    "dfAir['Date'] = pd.to_datetime(dfAir['Date'], format='%m/%d/%Y')\n",
    "\n",
    "# merge with sdfPUrplAir on Date\n",
    "df = dfFire.merge(dfAir, on='Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creat a plotly express line chart\n",
    "fig = px.line(dfAir, \n",
    "                x=\"Date\", \n",
    "                y=\"PM 2.5 (ug/m3)\", \n",
    "                title='Purple Air PM2.5'\n",
    "                            )\n",
    "\n",
    "fig.update_traces(hovertemplate=\"<br>\".join([\n",
    "                    \"<b>%{y:,.0f} AQI</b>\",\n",
    "                    \"<i>seven day rolling average</i>\"\n",
    "                        ])+\"<extra></extra>\"\n",
    ")\n",
    "\n",
    "# add scatter trace to figure of points of dfMerge by date and mean_pm25\n",
    "fig.add_trace(go.Scatter(x=df['Date'], \n",
    "                         y=df['PM 2.5 (ug/m3)'], \n",
    "                         mode='markers',\n",
    "                         name='Fire Start Date', \n",
    "                         customdata=df[[\"Fire\", \"Acres\"]],\n",
    "                         hovertemplate=\"<br>\".join([\n",
    "                                        \"<b>%{customdata[0]} FIRE</b> started\",\n",
    "                                        \"<i>%{x}</i> and burned\",\n",
    "                                        \"<i>%{customdata[1]:,.0f} acres</i>\"\n",
    "                                            ])+\"<extra></extra>\"\n",
    "                                            ))\n",
    "                        \n",
    "# make the scatter plot markers larger\n",
    "fig.update_traces(marker=dict(size=12))\n",
    "\n",
    "# label points by FIRE_NAME\n",
    "for i, txt in enumerate(df['Fire']):\n",
    "    fig.add_annotation(x=df['Date'].iloc[i], y=df['PM 2.5 (ug/m3)'].iloc[i], \n",
    "                            text=txt+\" FIRE\", \n",
    "                            showarrow=True, \n",
    "                            align=\"center\",\n",
    "                            arrowhead=1,\n",
    "                            # arrowsize=1,\n",
    "                            # arrowwidth=2,\n",
    "                            arrowcolor=\"#636363\",\n",
    "                            ax=10,\n",
    "                            ay=-20,\n",
    "                            bordercolor=\"#c7c7c7\",\n",
    "                            borderwidth=1,\n",
    "                            borderpad=2,\n",
    "                            bgcolor=\"#ff7f0e\",\n",
    "                            opacity=0.8\n",
    "                            )\n",
    "\n",
    "# plot variables\n",
    "path_html=\"html/1.2.a_Purple_Air_v2.html\"\n",
    "div_id=\"1.2.a_Purple_Air_v2\"\n",
    "x=\"time_stamp\"\n",
    "y=\"moving_avg\"\n",
    "color=\"#FF5733\"\n",
    "color_sequence=[\"#023f64\"]\n",
    "sort=\"time_stamp\"\n",
    "orders=None\n",
    "x_title=\"Date\"\n",
    "y_title=\"Air Quality Index\"\n",
    "format=\",.0f\"\n",
    "# hovertemplate=\"%{y:,.0f}\"\n",
    "hovermode=\"x unified\"\n",
    "config = {\"displayModeBar\": False}\n",
    "\n",
    "fig.update_layout(\n",
    "        yaxis=dict(title=y_title),\n",
    "        xaxis=dict(title=x_title, showgrid=False),\n",
    "        hovermode=hovermode,\n",
    "        template=\"plotly_white\",\n",
    "        dragmode=False,\n",
    "        showlegend=False,\n",
    "        # title_font_family=\"Bell Topo Sans\",\n",
    "        title_font_color=\"black\",\n",
    "        title=dict(text=\"Air Quality Index and Significant Fire Events\", \n",
    "                    x=0.05, \n",
    "                    y=0.95,\n",
    "                    xanchor=\"left\",\n",
    "                    yanchor=\"top\",\n",
    "                    font=dict(size=18), \n",
    "                    automargin=True)\n",
    "    )\n",
    "\n",
    "# fig.update_traces(hovertemplate=hovertemplate)\n",
    "fig.update_yaxes(tickformat=format)\n",
    "\n",
    "# set color of scatter marker\n",
    "fig.update_traces(marker=dict(color=color))\n",
    "# set color of line\n",
    "fig.update_traces(line=dict(color=color_sequence[0]))\n",
    "\n",
    "fig.write_html(\n",
    "        config=config,\n",
    "        file=path_html,\n",
    "        include_plotlyjs=\"directory\",\n",
    "        div_id=div_id\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import read_file\n",
    "\n",
    "# Trendline\n",
    "def trendline(\n",
    "    df,\n",
    "    path_html,\n",
    "    div_id,\n",
    "    x,\n",
    "    y,\n",
    "    color,\n",
    "    color_sequence,\n",
    "    sort,\n",
    "    orders,\n",
    "    x_title,\n",
    "    y_title,\n",
    "    format,\n",
    "    hovertemplate,\n",
    "    markers,\n",
    "    hover_data,\n",
    "    tickvals,\n",
    "    ticktext,\n",
    "    tickangle,\n",
    "    hovermode,\n",
    "):\n",
    "    df = df.sort_values(by=sort)\n",
    "    config = {\"displayModeBar\": False}\n",
    "    fig = px.line(\n",
    "        df,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        color=color,\n",
    "        color_discrete_sequence=color_sequence,\n",
    "        category_orders=orders,\n",
    "        markers=markers,\n",
    "        hover_data=hover_data,\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        yaxis=dict(title=y_title),\n",
    "        xaxis=dict(title=x_title, showgrid=False),\n",
    "        hovermode=hovermode,\n",
    "        template=\"plotly_white\",\n",
    "        dragmode=False,\n",
    "    )\n",
    "    fig.update_traces(hovertemplate=hovertemplate)\n",
    "    fig.update_yaxes(tickformat=format)\n",
    "    fig.update_xaxes(\n",
    "        tickvals=tickvals,\n",
    "        ticktext=ticktext,\n",
    "        tickangle=tickangle,\n",
    "    )\n",
    "    fig.write_html(\n",
    "        config=config,\n",
    "        file=path_html,\n",
    "        include_plotlyjs=\"directory\",\n",
    "        div_id=div_id,\n",
    "    )\n",
    "def calcAQI(Cp, Ih, Il, BPh, BPl):\n",
    "    a = Ih - Il\n",
    "    b = BPh - BPl\n",
    "    c = Cp - BPl\n",
    "    val = round((a / b) * c + Il)\n",
    "    return val\n",
    "\n",
    "def get_data_purple_air():\n",
    "    df = read_file(\"data/daily_averaged_values.csv\")\n",
    "    df[\"AQI\"] = np.where(\n",
    "        df[\"daily_mean_25pm\"] > 350.5,\n",
    "        calcAQI(df[\"daily_mean_25pm\"], 500, 401, 500.4, 350.5),\n",
    "        np.where(\n",
    "            df[\"daily_mean_25pm\"] > 250.5,\n",
    "            calcAQI(df[\"daily_mean_25pm\"], 400, 301, 350.4, 250.5),\n",
    "            np.where(\n",
    "                df[\"daily_mean_25pm\"] > 150.5,\n",
    "                calcAQI(df[\"daily_mean_25pm\"], 300, 201, 250.4, 150.5),\n",
    "                np.where(\n",
    "                    df[\"daily_mean_25pm\"] > 55.5,\n",
    "                    calcAQI(df[\"daily_mean_25pm\"], 200, 151, 150.4, 55.5),\n",
    "                    np.where(\n",
    "                        df[\"daily_mean_25pm\"] > 35.5,\n",
    "                        calcAQI(df[\"daily_mean_25pm\"], 150, 101, 55.4, 35.5),\n",
    "                        np.where(\n",
    "                            df[\"daily_mean_25pm\"] > 12.1,\n",
    "                            calcAQI(df[\"daily_mean_25pm\"], 100, 51, 35.4, 12.1),\n",
    "                            np.where(\n",
    "                                df[\"daily_mean_25pm\"] >= 0,\n",
    "                                calcAQI(df[\"daily_mean_25pm\"], 50, 0, 12, 0),\n",
    "                                9999999,\n",
    "                            ),\n",
    "                        ),\n",
    "                    ),\n",
    "                ),\n",
    "            ),\n",
    "        ),\n",
    "    )\n",
    "    df[\"moving_avg\"] = df[\"AQI\"].rolling(window=7).mean()\n",
    "    # df[\"time_stamp\"] = pd.to_datetime(df[\"time_stamp\"])\n",
    "    # df.set_index('time_stamp', inplace=True)\n",
    "    # weekly_avg = df.resample('W').mean().reset_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def plot_purple_air_v2(df1, df2):\n",
    "    trendline(\n",
    "        df1,\n",
    "        path_html=\"html/1.2.a_Purple_Air_v2.html\",\n",
    "        div_id=\"1.2.a_Purple_Air_v2\",\n",
    "        x=\"time_stamp\",\n",
    "        y=\"moving_avg\",\n",
    "        color=None,\n",
    "        color_sequence=[\"#023f64\", \"#7ebfb5\", \"#a48352\", \"#fc9a61\", \"#A48794\", \"#b83f5d\"],\n",
    "        sort=\"time_stamp\",\n",
    "        orders=None,\n",
    "        x_title=\"Time\",\n",
    "        y_title=\"AQI (rolling average)\",\n",
    "        format=\",.0f\",\n",
    "        hovertemplate=\"%{y:,.0f}\",\n",
    "        markers=False,\n",
    "        hover_data=None,\n",
    "        tickvals=None,\n",
    "        ticktext=None,\n",
    "        tickangle=None,\n",
    "        hovermode=\"x\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dfAIr = get_data_purple_air()\n",
    "plot_purple_air_v2(dfAir, dfFire)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lake Tahoe water level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# tahoe city site number\n",
    "site_number = '10337000'\n",
    "\n",
    "# USGS API URL\n",
    "url = f'https://waterservices.usgs.gov/nwis/iv/?format=json&sites={site_number}&parameterCd=00065&startDT=2021-01-01&endDT=2021-01-30'\n",
    "\n",
    "# function to get data from USGS API as dataframe\n",
    "def get_usgs_data(days):\n",
    "    site_number = 10337000\n",
    "    \n",
    "    # Calculate the start and end dates based on the selected time range\n",
    "    end_date = datetime.now()\n",
    "    start_date = end_date - timedelta(days=days)\n",
    "    \n",
    "    start_date_str = start_date.strftime('%Y-%m-%d')\n",
    "    end_date_str = end_date.strftime('%Y-%m-%d')\n",
    "    \n",
    "    url = f'https://waterservices.usgs.gov/nwis/iv/?format=json&sites={site_number}&parameterCd=00065&startDT={start_date_str}&endDT={end_date_str}'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "\n",
    "    time_series_data = data['value']['timeSeries'][0]['values'][0]['value']\n",
    "\n",
    "    df = pd.DataFrame(time_series_data)\n",
    "    df['value'] = pd.to_numeric(df['value'])\n",
    "    df['dateTime'] = pd.to_datetime(df['dateTime'])\n",
    "    df['value'] = df['value'] + 6220\n",
    "    return df\n",
    "\n",
    "# create plot of lake level\n",
    "def update_chart(selected_days):\n",
    "    df = get_usgs_data(selected_days)\n",
    "\n",
    "    fig = px.line(df, x='dateTime', y='value', title='Lake Tahoe Water Level')\n",
    "    fig.update_xaxes(title_text='Time')\n",
    "    fig.update_yaxes(title_text='Water Level (ft)')\n",
    "\n",
    "    return fig\n",
    "\n",
    "# create plot of lake level\n",
    "update_chart(6000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The USGS Site at Tahoe City only has data for the last 5968 days (~16.3 years) October 2007\n",
    "days = 5968  \n",
    "df = get_usgs_data(days)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Annual average water temperature, including surface water temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def get_all_temp_midlake():\n",
    "    # get all data from lake temp URL\n",
    "    # get the date from seven days ago using datetime\n",
    "    start = datetime.now() - timedelta(days=365)\n",
    "    start = start.strftime('%Y%m%d')\n",
    "    end = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    dfMerge = pd.DataFrame()\n",
    "\n",
    "    # start = \"20240130\"\n",
    "    # end = \"20240202\"\n",
    "    ids= [1,2,3,4]\n",
    "\n",
    "    for id in ids: \n",
    "        lakeTempURL = f\"https://tepfsail50.execute-api.us-west-2.amazonaws.com/v1/report/nasa-tb?rptdate={start}&rptend={end}&id={id}\"\n",
    "        # get all data from lake temp URL using f string\n",
    "        response = requests.get(lakeTempURL)\n",
    "        df = pd.DataFrame(response.json())\n",
    "        # concat dataframes to dfMerge\n",
    "        dfMerge = pd.concat([dfMerge, df])\n",
    "        # convert LS_Temp_Avg to float and farenheit\n",
    "        dfMerge[\"RBR_0p5_m\"] = dfMerge[\"RBR_0p5_m\"].astype(float)\n",
    "        dfMerge[\"RBR_0p5_F\"] = dfMerge[\"RBR_0p5_m\"] * 9/5 + 32\n",
    "    return dfMerge\n",
    "\n",
    "def get_all_temp_shore():\n",
    "    # get all data from lake temp URL\n",
    "    # get the date from seven days ago using datetime\n",
    "    start = datetime.now() - timedelta(days=7)\n",
    "    start = start.strftime('%Y%m%d')\n",
    "    end = datetime.now().strftime('%Y%m%d')\n",
    "\n",
    "    dfMerge = pd.DataFrame()\n",
    "\n",
    "    ids= [1,2,3,4,5,6,7,8,9,10,11]\n",
    "\n",
    "    for id in ids: \n",
    "        lakeTempURL = f\"https://tepfsail50.execute-api.us-west-2.amazonaws.com/v1/report/ns-station-range?rptdate={start}&rptend={end}&id={id}\"\n",
    "        # get all data from lake temp URL using f string\n",
    "        response = requests.get(lakeTempURL)\n",
    "        df = pd.DataFrame(response.json())\n",
    "        # concat dataframes to dfMerge\n",
    "        dfMerge = pd.concat([dfMerge, df])\n",
    "        # convert LS_Temp_Avg to float and farenheit\n",
    "        dfMerge[\"LS_Temp_Avg\"] = dfMerge[\"LS_Temp_Avg\"].astype(float)\n",
    "        dfMerge[\"LS_Temp_Avg_F\"] = dfMerge[\"LS_Temp_Avg\"] * 9/5 + 32\n",
    "    return dfMerge\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_all_temp_midlake()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df = get_all_temp_midlake()\n",
    "\n",
    "# df['date'] = pd.to_datetime(df['TmStamp'])\n",
    "# get mean RBR_0p5_F by date\n",
    "df = df.groupby('TmStamp')['RBR_0p5_F'].mean().reset_index()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_all_temp_shore()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import requests\n",
    "from meteostat import Daily, Point\n",
    "\n",
    "from utils import get_fs_data, read_file, scatterplot, stackedbar, trendline\n",
    "\n",
    "def plot_lake_temp(df):\n",
    "    trendline(\n",
    "        df,\n",
    "        path_html=\"html/1.3.b_Lake_Temp.html\",\n",
    "        div_id=\"1.3.b_Lake_Temp\",\n",
    "        x=\"TmStamp\",\n",
    "        y=\"RBR_0p5_F\",\n",
    "        color=None,\n",
    "        color_sequence=[\"#023f64\"],\n",
    "        sort=\"TmStamp\",\n",
    "        orders=None,\n",
    "        x_title=\"Date\",\n",
    "        y_title=\"Average Lake Surface Temperature\",\n",
    "        format=\".1f\",\n",
    "        hovertemplate=\"%{y:.2f}\",\n",
    "        markers=False,\n",
    "        hover_data=None,\n",
    "        tickvals=None,\n",
    "        ticktext=None,\n",
    "        tickangle=None,\n",
    "        hovermode=\"x\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lake_temp(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# get data from TERC service\n",
    "lakeTempURL = \"https://tepfsail50.execute-api.us-west-2.amazonaws.com/v1/report/ns-station-range?rptdate=20240130&rptend=20240202&id=4\"\n",
    "# get data from TERC\n",
    "response = requests.get(lakeTempURL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lake clarity measured by Secchi Depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# secchi depth data\n",
    "secchiDepth    = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/14\"\n",
    "secchiDepthAll = \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/13\"\n",
    "secchiDepth    = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/125\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total precipitation in water per year, extreme precipitation, and snow as a fraction of annual precipitation\n",
    "* https://www.ncei.noaa.gov/cdo-web/\n",
    "* https://www.weather.gov/documentation/services-web-api\n",
    "* https://www.ncdc.noaa.gov/cdo-web/api/v2/\n",
    "\n",
    "##### ended up using \n",
    "* https://cssl.berkeley.edu/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# folder with individual files\n",
    "folder = r\"C:\\Users\\mbindl\\Downloads\\doi_10_6078_D1941T__v20210622\"\n",
    "\n",
    "# create a list to store the data\n",
    "data = []\n",
    "# loop through the files in the folder\n",
    "for file in os.listdir(folder):\n",
    "    # read the file into a DataFrame\n",
    "    df = pd.read_csv(os.path.join(folder, file))\n",
    "    # add the DataFrame to the list\n",
    "    data.append(df)\n",
    "\n",
    "# concatenate the data into a single DataFrame\n",
    "combined_data = pd.concat(data)\n",
    "\n",
    "# change field names to have _ instead of spaces\n",
    "combined_data.columns = combined_data.columns.str.replace(' ', '_')\n",
    "# remove leading and trailing whitespace from field names and change % te Pct\n",
    "combined_data.columns = combined_data.columns.str.strip().str.replace('%', 'Pct')\n",
    "# remove () from field names\n",
    "combined_data.columns = combined_data.columns.str.replace('(', '').str.replace(')', '')\n",
    "# remove - from field names\n",
    "combined_data.columns = combined_data.columns.str.replace('-', '_')\n",
    "# change filed name 24-hour_Total_Precip_mm to Full_Day_Total_Precip_mm\n",
    "combined_data.rename(columns={'24_hour_Total_Precip_mm': 'Full_Day_Total_Precip_mm'}, inplace=True)\n",
    "\n",
    "# make sure all '' are replaced with NaN\n",
    "combined_data = combined_data.replace('', pd.NA)\n",
    "# replace '--' with NaN\n",
    "combined_data = combined_data.replace('--', np.NaN)\n",
    "# replace 'T' in 24_hour_Total_Precip_in with 0\n",
    "combined_data['Full_Day_Total_Precip_mm'] = combined_data['Full_Day_Total_Precip_mm'].replace('T', 0)\n",
    "# replace 'T' and '.' in New_Snow_cm with 0\n",
    "combined_data['New_Snow_cm'] = combined_data['New_Snow_cm'].replace('T', 0)\n",
    "combined_data['New_Snow_cm'] = combined_data['New_Snow_cm'].replace('.', 0)\n",
    "# replace 'T' and '.' in Season_Total_Snow_cm with 0\n",
    "combined_data['Season_Total_Snow_cm'] = combined_data['Season_Total_Snow_cm'].replace('T', 0)\n",
    "# replace 'T' and '.' in Snowpack_depth_cm with 0\n",
    "combined_data['Snowpack_depth_cm'] = combined_data['Snowpack_depth_cm'].replace('T', 0)\n",
    "# replace \"0'0 in Snow_Water_Equivalent_cm with 0\n",
    "combined_data['Snow_Water_Equivalent_cm'] = combined_data['Snow_Water_Equivalent_cm'].replace(\"0'0\", 0)\n",
    "# replace 'T' in Snow_Water_Equivalent_cm with 0\n",
    "combined_data['Snow_Water_Equivalent_cm'] = combined_data['Snow_Water_Equivalent_cm'].replace('T', 0)\n",
    "\n",
    "# convert fields to float\n",
    "combined_data['Pct_of_Precip_as_Snow'] = combined_data['Pct_of_Precip_as_Snow'].astype(float)\n",
    "combined_data['Pct_of_Precip_as_Rain'] = combined_data['Pct_of_Precip_as_Rain'].astype(float)\n",
    "combined_data['Full_Day_Total_Precip_mm'] = combined_data['Full_Day_Total_Precip_mm'].astype(float)\n",
    "combined_data['Air_Temp_Max_C'] = combined_data['Air_Temp_Max_C'].astype(float)\n",
    "combined_data['Air_Temp_Min_C'] = combined_data['Air_Temp_Min_C'].astype(float)\n",
    "combined_data['New_Snow_cm'] = combined_data['New_Snow_cm'].astype(float)\n",
    "combined_data['Season_Total_Snow_cm'] = combined_data['Season_Total_Snow_cm'].astype(float)\n",
    "combined_data['Snowpack_depth_cm'] = combined_data['Snowpack_depth_cm'].astype(float)\n",
    "combined_data['Snow_Water_Equivalent_cm'] = combined_data['Snow_Water_Equivalent_cm'].astype(float)\n",
    "\n",
    "# change Date to datetime\n",
    "combined_data['Date'] = pd.to_datetime(combined_data['Date'], format='mixed', errors='coerce')\n",
    "# add month and year columns\n",
    "combined_data['Month'] = combined_data['Date'].dt.month\n",
    "combined_data['Year'] = combined_data['Date'].dt.year\n",
    "# combined month and year into one column\n",
    "combined_data['Month_Year'] = combined_data['Date'].dt.to_period('M')\n",
    "# combined month and year into one column\n",
    "combined_data['Month_Year'] = combined_data.Month_Year.dt.strftime('%Y-%m')\n",
    "\n",
    "# save to csv\n",
    "combined_data.to_csv(os.path.join(folder,'CentralSierraSnowLab_AllData.csv'), index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.features import FeatureLayer\n",
    "import pandas as pd\n",
    "# get data from service as spatially enabled dataframe\n",
    "url = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/145\"\n",
    "fl = FeatureLayer(url)\n",
    "sdf = fl.query().sdf\n",
    "\n",
    "# cast Pct_of_Precip_as_Snow to float\n",
    "sdf['Pct_of_Precip_as_Snow'] = sdf['Pct_of_Precip_as_Snow'].astype(float)\n",
    "# cast Pct_of_Precip_as_Rain to float\n",
    "sdf['Pct_of_Precip_as_Rain'] = sdf['Pct_of_Precip_as_Rain'].astype(float)\n",
    "\n",
    "# new field of total snow as water equivalent\n",
    "sdf['Daily_Precip_Rain_mm'] = sdf.Full_Day_Total_Precip_mm * (sdf.Pct_of_Precip_as_Rain/100)\n",
    "sdf['Daily_Precip_Snow_mm'] = sdf.Full_Day_Total_Precip_mm * (sdf.Pct_of_Precip_as_Snow/100)\n",
    "\n",
    "dfYearly = sdf.groupby('Year').agg({'Daily_Precip_Rain_mm': 'sum', 'Daily_Precip_Snow_mm': 'sum'}).reset_index()\n",
    "\n",
    "dfYearly['Total_Precip_mm'] = dfYearly['Daily_Precip_Rain_mm'] + dfYearly['Daily_Precip_Snow_mm']\n",
    "\n",
    "# create percent field for Daily_Precip_Rain_mm by dividing by Full_Day_Total_Precip_mm\n",
    "dfYearly['Pct_of_Precip_as_Rain'] = (dfYearly['Daily_Precip_Rain_mm'] / dfYearly['Total_Precip_mm']) * 100\n",
    "# create percent field for Daily_Precip_Snow_mm by dividing by Full_Day_Total_Precip_mm\n",
    "dfYearly['Pct_of_Precip_as_Snow'] = (dfYearly['Daily_Precip_Snow_mm'] / dfYearly['Total_Precip_mm']) * 100\n",
    "\n",
    "# drop all years before 1987\n",
    "dfYearly = dfYearly[dfYearly['Year'] >= 1987]\n",
    "\n",
    "# convert total precip to inches\n",
    "dfYearly['Total_Precip_in'] = dfYearly['Total_Precip_mm'] * 0.0393701\n",
    "\n",
    "\n",
    "dfYearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plotly express bar chart\n",
    "import plotly.express as px\n",
    "\n",
    "# create stacked bar by year of percent of precip as rain vs snow\n",
    "fig = px.bar(dfYearly, x='Year', y=['Pct_of_Precip_as_Rain', 'Pct_of_Precip_as_Snow'], title='Percent of Precipitation as Rain vs Snow by Year')\n",
    "fig.update_layout(barmode='stack')\n",
    "\n",
    "# add second y axis with total precipitation\n",
    "fig.update_layout(yaxis2=dict(title='Total Precipitation (in)', overlaying='y', side='right'))\n",
    "fig.add_scatter(x=dfYearly['Year'], y=dfYearly['Total_Precip_in'], mode='lines', name='Total Precipitation (in)', yaxis=\"y2\")\n",
    "\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group the daily data by month and year and get the sum of the daily precip full day total, rain, and snow\n",
    "sdfMonthly = sdf.groupby('Month_Year').agg({'Full_Day_Total_Precip_mm': 'sum', 'Daily_Precip_Rain_mm': 'sum', 'Daily_Precip_Snow_mm': 'sum'}).reset_index()\n",
    "\n",
    "# create percent field for Daily_Precip_Rain_mm by dividing by Full_Day_Total_Precip_mm\n",
    "sdfMonthly['Pct_of_Precip_as_Rain'] = (sdfMonthly['Daily_Precip_Rain_mm'] / sdfMonthly['Full_Day_Total_Precip_mm']) * 100\n",
    "# create percent field for Daily_Precip_Snow_mm by dividing by Full_Day_Total_Precip_mm\n",
    "sdfMonthly['Pct_of_Precip_as_Snow'] = (sdfMonthly['Daily_Precip_Snow_mm'] / sdfMonthly['Full_Day_Total_Precip_mm']) * 100\n",
    "\n",
    "\n",
    "# drop any rows with NaN or where Full_Day_Total_Precip_mm is greater than 0 and Daily_Precip_Rain_mm and Daily_Precip_Snow_mm are 0\n",
    "sdfMonthly = sdfMonthly.dropna(subset=['Full_Day_Total_Precip_mm'])\n",
    "sdfMonthly = sdfMonthly[(sdfMonthly['Full_Day_Total_Precip_mm'] > 0) & ((sdfMonthly['Daily_Precip_Rain_mm'] > 0) | (sdfMonthly['Daily_Precip_Snow_mm'] > 0))]\n",
    "\n",
    "# drop rows where month_year ends with 05, 06, 07, 08, 09\n",
    "sdfMonthly = sdfMonthly[~sdfMonthly.Month_Year.str.endswith(('05', '06', '07', '08', '09'))]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly import graph_objects as go\n",
    "# create a plotly express stacked bar chart of Pct_of_Precip_as_Rain and Pct_of_Precip_as_Snow\n",
    "fig = px.bar(sdfMonthly, x='Month_Year', y=['Pct_of_Precip_as_Rain', 'Pct_of_Precip_as_Snow'], title='Monthly Precipitation Type')\n",
    "fig.update_layout(barmode='stack')\n",
    "\n",
    "# # add trendline of pct of precip as rain?\n",
    "\n",
    "# # add trace of line for total snow\n",
    "# fig.add_trace(go.Scatter(x=sdfMonthly['Month_Year'], y=sdfMonthly['Full_Day_Total_Precip_mm'], mode='lines', name='Total Precipitation (mm)'))\n",
    "\n",
    "# configure second y axis for total snow so that the right is 0 to 100 and the left is 0 to 1000\n",
    "\n",
    "# set y axis to be 0 to 100\n",
    "fig.update_yaxes(range=[0, 100], title_text='Percent of Precipitation')\n",
    "# add second y axis\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create scatter chart as lines of total snow by month\n",
    "fig = px.line(sdfMonthly, x='Month_Year', y=['Full_Day_Total_Precip_mm', 'Daily_Precip_Rain_mm', 'Daily_Precip_Snow_mm'], title='Monthly Precipitation')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create a plotly express line chart\n",
    "fig = px.line(df, x=\"Month_Year\", y=\"Full_Day_Total_Precip_mm\", title='Total Full Day Precipitation')\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get total snowfall for each month and year\n",
    "monthly_snow = combined_data.groupby(['Month_Year']).agg({'Season_Total_Snow_cm': 'sum'}).reset_index()\n",
    "monthly_snow.Month_Year = monthly_snow.Month_Year.dt.strftime('%Y-%m')\n",
    "# create a plotly express line chart\n",
    "fig = px.line(monthly_snow, x=\"Month_Year\", y=\"Season_Total_Snow_cm\", title='Monthly Snowfall')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acres of forest fuels reduction treated for wildfire in high-risk areas, map of prescribed fire treatment projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get EIP indicator as dataframe\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# EIP Service for Acres Treated\n",
    "eipForestTreatments = \"https://www.laketahoeinfo.org/WebServices/GetReportedEIPIndicatorProjectAccomplishments/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476/19\"\n",
    "\n",
    "# TRCD service ## Doesnt match EIP data ## from 2022 used in map though\n",
    "trcdLayer = \"https://services6.arcgis.com/1KtlSd2mklZMBKaz/ArcGIS/rest/services/Tahoe_Forest_Fuels_Tx_OFFICIAL_Public_View/FeatureServer/0\"\n",
    "\n",
    "# get the data\n",
    "dfTreatments = pd.read_json(eipForestTreatments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTreatments.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTreatments.to_csv(\"EIP_ForestHealthTreatments.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# get EIP indicator as dataframe\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# EIP Service for Acres Treated\n",
    "eipForestTreatments = \"https://www.laketahoeinfo.org/WebServices/GetReportedEIPIndicatorProjectAccomplishments/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476/19\"\n",
    "\n",
    "# TRCD service ## Doesnt match EIP data ## from 2022 used in map though\n",
    "trcdLayer = \"https://services6.arcgis.com/1KtlSd2mklZMBKaz/ArcGIS/rest/services/Tahoe_Forest_Fuels_Tx_OFFICIAL_Public_View/FeatureServer/0\"\n",
    "\n",
    "# get the data\n",
    "dfTreatments = pd.read_json(eipForestTreatments)\n",
    "\n",
    "# display the data\n",
    "dfTreatments.head()\n",
    "# group treatments by year\n",
    "dfTreatments = dfTreatments.groupby(['IndicatorProjectYear']).sum().reset_index()\n",
    "dfTreatments.head()\n",
    "# create plotly figure of forest treatments by year\n",
    "fig = px.bar(dfTreatments, x='IndicatorProjectYear', y='IndicatorProjectValue', title='Forest Treatments by Year')\n",
    "fig.update_xaxes(title_text='Year')\n",
    "fig.update_yaxes(title_text='Acres Treated')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree species diversity and increasing old growth forest "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from utils import get_fs_data, get_fs_data_spatial, stackedbar, trendline\n",
    "dfVeg = get_fs_data(\n",
    "    \"https://maps.trpa.org/server/rest/services/LTInfo_Monitoring/MapServer/91\"\n",
    "    )\n",
    "    \n",
    "dfVeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# veg data\n",
    "vegLayer = \"https://maps.trpa.org/server/rest/services/Vegetation_Type/MapServer/0\"\n",
    "\n",
    "# Gets data from the TRPA server\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    # return data frame\n",
    "    return all_data\n",
    "\n",
    "dfVeg = get_fs_data(vegLayer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets spatially enabled dataframe from TRPA server\n",
    "def get_fs_data_spatial(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query().sdf\n",
    "    return query_result\n",
    "\n",
    "# veg data\n",
    "vegLayer = \"https://maps.trpa.org/server/rest/services/Vegetation_Type/MapServer/0\"\n",
    "\n",
    "dfVeg = get_fs_data_spatial(vegLayer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_veg_type(dfVeg):\n",
    "    # veg colors\n",
    "    # colors of the veg types\n",
    "    colors = ['#d7d79e','#9ed7c2','#cdf57a','#b4d79e', \n",
    "          '#ff0000', '#a5f57a','#00a820','#df73ff', \n",
    "          '#3e72b0','#2f3f56', '#a8a800']\n",
    "    \n",
    "    # summarize the data by Acres\n",
    "    dfVeg.TRPA_VegType.replace('', np.nan, inplace=True)\n",
    "    dfVegType = dfVeg.groupby(\"TRPA_VegType\")[\"Acres\"].sum().reset_index()\n",
    "\n",
    "    df = dfVegType.rename(columns={'TRPA_VegType':'Vegetation Type'})\n",
    "\n",
    "    df['Vegetation Type'] = df['Vegetation Type'].replace(['Cushion Plant'],'Cushion Plant/Rocky Outcrop')\n",
    "\n",
    "    fig = px.bar(df, y=\"Vegetation Type\", x=\"Acres\", color=\"Vegetation Type\", orientation=\"h\", hover_name=\"Vegetation Type\",\n",
    "                color_discrete_sequence=colors ,\n",
    "                title=\"Vegetation Abundance\"\n",
    "                )\n",
    "\n",
    "    fig.update_traces(hovertemplate='<b>%{x:,.0f}</b> acres<extra></extra>')\n",
    "\n",
    "    # set style variables\n",
    "    template = 'plotly_white'\n",
    "    font     = 'Calibri'\n",
    "\n",
    "    fig.update_layout(font_family=font,\n",
    "                        template=template,\n",
    "                        showlegend=False,\n",
    "                        hovermode=\"y unified\",\n",
    "                        yaxis={'categoryorder':'total ascending'})\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# colors of the veg types\n",
    "colors = ['#d7d79e','#9ed7c2','#cdf57a','#b4d79e', \n",
    "          '#ff0000', '#a5f57a','#00a820','#df73ff', \n",
    "          '#3e72b0','#2f3f56', '#a8a800']\n",
    "\n",
    "# current species distribution Use \"TRPA_Veg_Type\" to display the current species distribution\n",
    "speciesLayer = \"https://maps.trpa.org/server/rest/services/Vegetation_Type/MapServer/0\"\n",
    "\n",
    "# get the feature layer as a spatially enabled dataframe\n",
    "dfVeg = FeatureLayer(speciesLayer).query().sdf\n",
    "\n",
    "# summarize the data by Acres\n",
    "dfVeg.TRPA_VegType.replace('', np.nan, inplace=True)\n",
    "dfVegType = dfVeg.groupby(\"TRPA_VegType\")[\"Acres\"].sum().reset_index()\n",
    "\n",
    "df = dfVegType.rename(columns={'TRPA_VegType':'Vegetation Type'})\n",
    "\n",
    "df['Vegetation Type'] = df['Vegetation Type'].replace(['Cushion Plant'],'Cushion Plant/Rocky Outcrop')\n",
    "\n",
    "fig = px.bar(df, y=\"Vegetation Type\", x=\"Acres\", color=\"Vegetation Type\", orientation=\"h\", hover_name=\"Vegetation Type\",\n",
    "             color_discrete_sequence=colors ,\n",
    "             title=\"Vegetation Abundance\"\n",
    "            )\n",
    "\n",
    "fig.update_traces(hovertemplate='<b>%{x:,.0f}</b> acres<extra></extra>')\n",
    "\n",
    "# set style variables\n",
    "template = 'plotly_white'\n",
    "font     = 'Calibri'\n",
    "\n",
    "fig.update_layout(font_family=font,\n",
    "                    template=template,\n",
    "                    showlegend=False,\n",
    "                    hovermode=\"y unified\",\n",
    "                    yaxis={'categoryorder':'total ascending'})\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of fire by low, moderate, & high severity by management zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spatial analysis for fire severity\n",
    "import arcpy\n",
    "\n",
    "# reclass Low Severity flame length raster\n",
    "with arcpy.EnvManager(scratchWorkspace=r\"F:\\GIS\\PROJECTS\\ForestHealth_Intiative\\ThresholdUpdate\\Data\\ForestHealth_ThresholdUpdate.gdb\"):\n",
    "    out_raster = arcpy.sa.Reclassify(\n",
    "        in_raster=r\"F:\\\\GIS\\\\PROJECTS\\\\ForestHealth_Intiative\\\\ThresholdUpdate\\\\Data\\\\ForestHealth_ThresholdUpdate.gdb\\FunctionalFire_ProbLowSeverity_2022_ACCEL_30m_Tahoe\",\n",
    "        reclass_field=\"Value\",\n",
    "        remap=\"0 0.600000 0;0.600000 1 1\",\n",
    "        missing_values=\"DATA\"\n",
    "    )\n",
    "    out_raster.save(r\"F:\\GIS\\PROJECTS\\ForestHealth_Intiative\\ThresholdUpdate\\Data\\ForestHealth_ThresholdUpdate.gdb\\FunctionalFire_Reclass_Low_60thPercentile\")\n",
    "\n",
    "# convert reclassified raster to polygon\n",
    "arcpy.conversion.RasterToPolygon(\n",
    "    in_raster=r\"Fire Severity\\Low Severity Fire\",\n",
    "    out_polygon_features=r\"F:\\GIS\\PROJECTS\\ForestHealth_Intiative\\ThresholdUpdate\\Data\\ForestHealth_ThresholdUpdate.gdb\\FunctionalFire_ProbableLowSeverityFire\",\n",
    "    simplify=\"NO_SIMPLIFY\",\n",
    "    raster_field=\"Value\",\n",
    "    create_multipart_features=\"SINGLE_OUTER_PART\",\n",
    "    max_vertices_per_feature=None\n",
    ")\n",
    "\n",
    "# identity analysis\n",
    "arcpy.analysis.Identity(\n",
    "    in_features=r\"Boundaries\\Forest Management Zone\",\n",
    "    identity_features=\"FunctionalFire_ProbableLowSeverityFire\",\n",
    "    out_feature_class=r\"F:\\GIS\\PROJECTS\\ForestHealth_Intiative\\ThresholdUpdate\\Data\\ForestHealth_ThresholdUpdate.gdb\\ForestManagementZon_Identity_LowSeverity\",\n",
    "    join_attributes=\"ALL\",\n",
    "    cluster_tolerance=None,\n",
    "    relationship=\"NO_RELATIONSHIPS\"\n",
    ")\n",
    "\n",
    "# Calculate the area in acres for each feature\n",
    "arcpy.management.CalculateGeometryAttributes(\n",
    "    in_features=r\"Fire Severity\\ForestManagementZon_Identity_LowSeverity\",\n",
    "    geometry_property=\"Acres AREA\",\n",
    "    length_unit=\"\",\n",
    "    area_unit=\"ACRES_US\",\n",
    "    coordinate_system='PROJCS[\"NAD_1983_UTM_Zone_10N\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",500000.0],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",-123.0],PARAMETER[\"Scale_Factor\",0.9996],PARAMETER[\"Latitude_Of_Origin\",0.0],UNIT[\"Meter\",1.0]]',\n",
    "    coordinate_format=\"SAME_AS_INPUT\"\n",
    ")\n",
    "\n",
    "# export table to database\n",
    "arcpy.conversion.ExportTable(\n",
    "    in_table=r\"Fire Severity\\ForestManagementZon_Identity_LowSeverity\",\n",
    "    out_table=r\"F:\\GIS\\DB_CONNECT\\Tabular.sde\\SDE.ClimateResilience_ProbabilityLowSeverityFire_by_ForestManagmentZone\",\n",
    "    where_clause=\"\",\n",
    "    use_field_alias_as_name=\"NOT_USE_ALIAS\",\n",
    "    sort_field=None\n",
    ")\n",
    "\n",
    "# reclass High Severity flame length raster\n",
    "with arcpy.EnvManager(scratchWorkspace=r\"F:\\GIS\\PROJECTS\\ForestHealth_Intiative\\ThresholdUpdate\\Data\\ForestHealth_ThresholdUpdate.gdb\"):\n",
    "    out_raster = arcpy.sa.Reclassify(\n",
    "        in_raster=r\"F:\\\\GIS\\\\PROJECTS\\\\ForestHealth_Intiative\\\\ThresholdUpdate\\\\Data\\\\ForestHealth_ThresholdUpdate.gdb\\FunctionalFire_ProbHighSeverity_2022_ACCEL_30m_Tahoe\",\n",
    "        reclass_field=\"Value\",\n",
    "        remap=\"0 0.600000 0;0.600000 1 1\",\n",
    "        missing_values=\"DATA\"\n",
    "    )\n",
    "    out_raster.save(r\"F:\\GIS\\PROJECTS\\ForestHealth_Intiative\\ThresholdUpdate\\Data\\ForestHealth_ThresholdUpdate.gdb\\FunctionalFire_Reclass_High_60thPercentile\")\n",
    "\n",
    "# convert reclassified raster to polygon\n",
    "arcpy.conversion.RasterToPolygon(\n",
    "    in_raster=r\"Fire Severity\\High Severity Fire\",\n",
    "    out_polygon_features=r\"F:\\GIS\\PROJECTS\\ForestHealth_Intiative\\ThresholdUpdate\\Data\\ForestHealth_ThresholdUpdate.gdb\\FunctionalFire_ProbableHighSeverityFire\",\n",
    "    simplify=\"NO_SIMPLIFY\",\n",
    "    raster_field=\"Value\",\n",
    "    create_multipart_features=\"SINGLE_OUTER_PART\",\n",
    "    max_vertices_per_feature=None\n",
    ")\n",
    "\n",
    "# identity analysis\n",
    "arcpy.analysis.Identity(\n",
    "    in_features=r\"Boundaries\\Forest Management Zone\",\n",
    "    identity_features=\"FunctionalFire_ProbableHighSeverityFire\",\n",
    "    out_feature_class=r\"F:\\GIS\\PROJECTS\\ForestHealth_Intiative\\ThresholdUpdate\\Data\\ForestHealth_ThresholdUpdate.gdb\\ForestManagementZon_Identity_HighSeverity\",\n",
    "    join_attributes=\"ALL\",\n",
    "    cluster_tolerance=None,\n",
    "    relationship=\"NO_RELATIONSHIPS\"\n",
    ")\n",
    "\n",
    "# Calculate the area in acres for each feature\n",
    "arcpy.management.CalculateGeometryAttributes(\n",
    "    in_features=r\"Fire Severity\\ForestManagementZon_Identity_HighSeverity\",\n",
    "    geometry_property=\"Acres AREA\",\n",
    "    length_unit=\"\",\n",
    "    area_unit=\"ACRES_US\",\n",
    "    coordinate_system='PROJCS[\"NAD_1983_UTM_Zone_10N\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",500000.0],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",-123.0],PARAMETER[\"Scale_Factor\",0.9996],PARAMETER[\"Latitude_Of_Origin\",0.0],UNIT[\"Meter\",1.0]]',\n",
    "    coordinate_format=\"SAME_AS_INPUT\"\n",
    ")\n",
    "\n",
    "# export table to database\n",
    "arcpy.conversion.ExportTable(\n",
    "    in_table=r\"Fire Severity\\ForestManagementZon_Identity_HighSeverity\",\n",
    "    out_table=r\"F:\\GIS\\DB_CONNECT\\Tabular.sde\\SDE.ClimateResilience_ProbabilityHighSeverityFire_by_ForestManagmentZone\",\n",
    "    where_clause=\"\",\n",
    "    use_field_alias_as_name=\"NOT_USE_ALIAS\",\n",
    "    sort_field=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# import packages\n",
    "from arcgis.features import FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the service URL\n",
    "highseverityURL = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/129\"\n",
    "lowseverityURL  = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/130\"\n",
    "\n",
    "# Create a FeatureLayer object\n",
    "feature_layer = FeatureLayer(highseverityURL)\n",
    "\n",
    "# Query the feature layer and convert the result to a Spatially Enabled DataFrame\n",
    "sdf_highseverity = feature_layer.query().sdf\n",
    "\n",
    "sdf_highseverity.head()\n",
    "# summarize the area of high and low severity fire by name and gridcode\n",
    "highseverity_summary = sdf_highseverity.groupby(['Name', 'gridcode'])['Acres'].sum().reset_index()\n",
    "\n",
    "# reclassify values of gridcode in new field called 'Severity' with 1 = \">60% chance of high severity fire\" and 0 = \"<60% chance of high severity fire\"\n",
    "highseverity_summary['Severity'] = np.where(highseverity_summary['gridcode'] == 1, \">60% chance of high severity fire\", \"<60% chance of high severity fire\")\n",
    "\n",
    "# Plot using Plotly Express to create a stacked bar chart of Severity by Name with the bar chart being 100% stacked\n",
    "fig = px.histogram(highseverity_summary, x='Name', y='Acres', color='Severity', title='High Severity Fire by Forest Management Zone', barnorm='percent',barmode='stack')\n",
    "# fig = px.bar(highseverity_summary, x='Name', y='Acres', color='Severity', title='High Severity Fire by Forest Management Zone', barmode='stack')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lowseverityURL  = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/130\"\n",
    "\n",
    "# Create a FeatureLayer object\n",
    "feature_layer = FeatureLayer(lowseverityURL)\n",
    "\n",
    "# Query the feature layer and convert the result to a Spatially Enabled DataFrame\n",
    "sdf_lowseverity = feature_layer.query().sdf\n",
    "\n",
    "sdf_lowseverity.head()\n",
    "# summarize the area of high and low severity fire by name and gridcode\n",
    "lowseverity_summary = sdf_lowseverity.groupby(['Name', 'gridcode'])['Acres'].sum().reset_index()\n",
    "\n",
    "# reclassify values of gridcode in new field called 'Severity' \n",
    "lowseverity_summary['Severity'] = np.where(lowseverity_summary['gridcode'] == 1, \">60% chance of low severity fire\", \"<60% chance of low severity fire\")\n",
    "\n",
    "# Plot using Plotly Express to create a stacked bar chart of Severity by Name with the bar chart being 100% stacked\n",
    "fig = px.histogram(lowseverity_summary, x='Name', y='Acres', color='Severity', title='Low Severity Fire by Forest Management Zone', barnorm='percent',barmode='stack')\n",
    "# fig = px.bar(highseverity_summary, x='Name', y='Acres', color='Severity', title='High Severity Fire by Forest Management Zone', barmode='stack')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data for Moderate Severity Fire does not exist in the service....USFS data is needed to complete the analysis, but they didnt provide it in the data package\n",
    "\n",
    "# we could get at this by doing the math's between low and high severity fire, but that would be a bit of a stretch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acres treated for aquatic invasive species. *We could gain additional clarity regarding this indicator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get EIP indicator as dataframe\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "# EIP Service for Acres Treated\n",
    "eipInvasive = \"https://www.laketahoeinfo.org/WebServices/GetReportedEIPIndicatorProjectAccomplishments/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476/15\"\n",
    "\n",
    "df = pd.read_json(eipInvasive)\n",
    "\n",
    "sum_df = df.groupby('IndicatorProjectYear')['IndicatorProjectValue'].cumsum()\n",
    "\n",
    "display(sum_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Acres of restored high-quality wetlands and meadows helping to store flood waters. *We are expecting additional clarity regarding this indicator.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get EIP indicator as dataframe\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "\n",
    "# EIP Service for SEZ Restored or Enhanced\n",
    "eipSEZRestored = \"https://www.laketahoeinfo.org/WebServices/GetReportedEIPIndicatorProjectAccomplishments/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476/9\"\n",
    "\n",
    "# GIS of SEZ Enchanced\n",
    "trpaSEZEnhancedRestored = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/126\"\n",
    "# GIS of SEZ Restored\n",
    "trpaSEZRestored = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/127\"\n",
    "# get the data\n",
    "df = pd.read_json(eipSEZRestored)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increased number of parcels with Stormwater Best Management Practices (BMPs) improvements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get BMP data as dataframe\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# # EIP Service for BMPs installed\n",
    "# bmpsInstalled = \"https://www.laketahoeinfo.org/WebServices/GetReportedEIPIndicatorProjectAccomplishments/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476/4\"\n",
    "\n",
    "# BMP map service from BMP database\n",
    "bmpsLayer = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/121\"\n",
    "\n",
    "# get the data\n",
    "feature_layer = FeatureLayer(bmpsLayer)\n",
    "\n",
    "# Query the feature layer and convert the result to a Spatially Enabled DataFrame\n",
    "sdf_bmps = feature_layer.query().sdf\n",
    "\n",
    "# total BMPs installed\n",
    "total_bmps = sdf_bmps['OBJECTID'].count()\n",
    "\n",
    "# filter total BMPs CertificateIssued = 1\n",
    "sdf_bmps_cert = sdf_bmps[sdf_bmps['CertificateIssued'] == 1]\n",
    "\n",
    "# total BMPs installed\n",
    "total_bmps_cert = sdf_bmps_cert['OBJECTID'].count()\n",
    "\n",
    "# create Year column\n",
    "sdf_bmps_cert['Year'] = pd.DatetimeIndex(sdf_bmps_cert['CertDate']).year\n",
    "\n",
    "# total BMPs installed per year\n",
    "total_bmps_cert_year = sdf_bmps_cert.groupby('Year')['OBJECTID'].count().reset_index()\n",
    "\n",
    "# create plotly figure of BMPs installed per year\n",
    "fig = px.bar(total_bmps_cert_year, x='Year', y='OBJECTID', title='BMPs Installed per Year')\n",
    "fig.update_xaxes(title_text='Year')\n",
    "fig.update_yaxes(title_text='BMPs Installed')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cumulutive sum of BMPs installed per year\n",
    "total_bmps_cert_year['Cumulative BMPs Installed'] = total_bmps_cert_year['OBJECTID'].cumsum()\n",
    "# create plotly figure of BMPs installed per year\n",
    "fig = px.bar(total_bmps_cert_year, x='Year', y='Cumulative BMPs Installed', title='BMPs Installed')\n",
    "fig.update_xaxes(title_text='Year')\n",
    "fig.update_yaxes(title_text='Cumulative BMPs Installed')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMP map service from BMP database\n",
    "bmpsLayer = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/121\"\n",
    "# get the data\n",
    "feature_layer = FeatureLayer(bmpsLayer)\n",
    "\n",
    "# Query the feature layer and convert the result to a Spatially Enabled DataFrame\n",
    "sdf_bmps = feature_layer.query().sdf\n",
    "\n",
    "# filter total BMPs CertificateIssued = 1\n",
    "sdf_bmps_cert = sdf_bmps[sdf_bmps['CertificateIssued'] == 1]\n",
    "\n",
    "# total developed parcels, use .loc to get EXISTING_LANDUSE does not equal \"Vacant\" or \"Open Space\"\n",
    "parcelsDeveloped = sdf_bmps.loc[~sdf_bmps['EXISTING_LANDUSE'].isin(['Vacant', 'Open Space'])]\n",
    "parcelsDeveloped = parcelsDeveloped['OBJECTID'].count()\n",
    "\n",
    "# total bmps certified by year compared to total developed parcels\n",
    "# create Year column\n",
    "sdf_bmps_cert['Year'] = pd.DatetimeIndex(sdf_bmps_cert['CertDate']).year\n",
    "bmpsCertByYear = sdf_bmps_cert.groupby('Year')['OBJECTID'].count().reset_index()\n",
    "bmpsCertByYear['Cumulative BMPs Installed'] = bmpsCertByYear['OBJECTID'].cumsum()\n",
    "bmpsCertByYear['Developed Parcels'] = parcelsDeveloped\n",
    "bmpsCertByYear['BMPs per Developed Parcel'] = bmpsCertByYear['Cumulative BMPs Installed'] / bmpsCertByYear['Developed Parcels']\n",
    "bmpsCertByYear['BMPs per Developed Parcel'] = bmpsCertByYear['BMPs per Developed Parcel'].round(2)\n",
    "\n",
    "# create plotly figure of BMPs installed per year\n",
    "fig = px.bar(bmpsCertByYear, x='Year', y='BMPs per Developed Parcel', title='BMPs Installed per Developed Parcel')\n",
    "fig.update_xaxes(title_text='Year')\n",
    "fig.update_yaxes(title_text='% BMPs Installed per Developed Parcel')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data_bmp():\n",
    "    # BMP map service from BMP database\n",
    "    bmpsLayer = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/121\"\n",
    "\n",
    "    # get data from map service\n",
    "    data = get_fs_data(bmpsLayer)\n",
    "\n",
    "    # select rows where BMPs CertificateIssued = 1 using .loc\n",
    "    data.loc[data['CertificateIssued'] == 1]\n",
    "\n",
    "    # create Year column\n",
    "    data['Year'] = pd.DatetimeIndex(data['CertDate']).year\n",
    "\n",
    "    # total bmps certified by year\n",
    "    bmpsCertByYear = data.groupby('Year')['OBJECTID'].count().reset_index()\n",
    "\n",
    "    # total developed parcels, get EXISTING_LANDUSE does not equal \"Vacant\" or \"Open Space\"\n",
    "    parcelsDeveloped = data.loc[~data['EXISTING_LANDUSE'].isin(['Vacant', 'Open Space'])]\n",
    "\n",
    "    # total developed parcels\n",
    "    bmpsCertByYear['Developed Parcels'] = parcelsDeveloped['OBJECTID'].count()\n",
    "\n",
    "    # cumulative sum of BMPs installed per year\n",
    "    bmpsCertByYear['Cumulative BMPs Installed'] = bmpsCertByYear['OBJECTID'].cumsum()\n",
    "\n",
    "    # BMPs installed per year compared to total developed parcels per year\n",
    "    bmpsCertByYear['BMPs per Developed Parcel'] = bmpsCertByYear['Cumulative BMPs Installed'] / bmpsCertByYear['Developed Parcels'].round(2)\n",
    "\n",
    "    # BMPs installed per year compared to total developed parcels per year but subtracting the BMPs installed from the total developed parcels\n",
    "    bmpsCertByYear['Developed Parcels Without a BMP'] = bmpsCertByYear['Developed Parcels'] - bmpsCertByYear['Cumulative BMPs Installed']\n",
    "    \n",
    "    # drop objectid\n",
    "    df = bmpsCertByYear.drop(columns=['OBJECTID'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_data_bmp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_bmp():\n",
    "    # BMP map service from BMP database\n",
    "    bmpsLayer = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/121\"\n",
    "    # get data from map service\n",
    "    data = get_fs_data(bmpsLayer)\n",
    "    # select rows where BMPs CertificateIssued = 1 (True)\n",
    "    data.loc[data['CertificateIssued'] == 1]\n",
    "    # create Year column\n",
    "    data['Year'] = pd.DatetimeIndex(data['CertDate']).year\n",
    "    # total bmps certified by year\n",
    "    bmpsCertByYear = data.groupby('Year')['OBJECTID'].count().reset_index()\n",
    "    # total developed parcel rows\n",
    "    parcelsDeveloped = data.loc[~data['EXISTING_LANDUSE'].isin(['Vacant', 'Open Space'])]\n",
    "    # set total developed parcels field\n",
    "    bmpsCertByYear['Developed Parcels'] = parcelsDeveloped['OBJECTID'].count()\n",
    "    # cumulative sum of BMPs installed per year\n",
    "    bmpsCertByYear['Total BMPs Installed'] = bmpsCertByYear['OBJECTID'].cumsum()\n",
    "    # BMPs installed per year compared to total developed parcels per year\n",
    "    bmpsCertByYear['BMPs per Developed Parcel'] = (bmpsCertByYear['Total BMPs Installed'] / bmpsCertByYear['Developed Parcels']).round(2)\n",
    "    # BMPs installed per year compared to total developed parcels per year but subtracting the BMPs installed from the total developed parcels\n",
    "    bmpsCertByYear['Developed Parcels Without a BMP'] = bmpsCertByYear['Developed Parcels'] - bmpsCertByYear['Total BMPs Installed']\n",
    "    # drop objectid\n",
    "    df = bmpsCertByYear.drop(columns=['OBJECTID'])\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BMP map service from BMP database\n",
    "bmpsLayer = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/121\"\n",
    "\n",
    "# get data from map service\n",
    "data = get_fs_data(bmpsLayer)\n",
    "# select rows where BMPs CertificateIssued = 1 (True)\n",
    "df = data.loc[data['CertificateIssued'] == 1]\n",
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create staked bar chart of BMPs installed per year compared to total developed parcels per year but subtracting the BMPs installed from the total developed parcels\n",
    "bmpsCertByYear['Developed Parcels Without a BMP'] = bmpsCertByYear['Developed Parcels'] - bmpsCertByYear['Cumulative BMPs Installed']\n",
    "\n",
    "bmpsCertByYear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create staked bar chart of BMPs installed per year compared to total developed parcels per year but subtracting the BMPs installed from the total developed parcels\n",
    "fig = px.bar(bmpsCertByYear, x='Year', y=['Cumulative BMPs Installed', 'Developed Parcels Without a BMP'], title='BMPs Installed per Developed Parcel')\n",
    "fig.update_xaxes(title_text='Year')\n",
    "fig.update_yaxes(title_text='BMPs Installed per Developed Parcel')\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increase in square feet of urban development treated by areawide stormwater infrastructure within key watersheds.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## reqeusted \"Year of Completion\" data for each area wide storm water treatment from Shay. TBD if this is available\n",
    "\n",
    "# area wide storm water treamtent layer # we can't do this analysis without the year of completion data\n",
    "areawideLayer  = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/120\"\n",
    "\n",
    "# developed area layers\n",
    "impervious2010Layer   = \"https://maps.trpa.org/server/rest/services/Impervious_Surface_2010/MapServer\"\n",
    "impervious2019Layer   = \"https://maps.trpa.org/server/rest/services/Impervious_Surface_2019/MapServer\"\n",
    "imperviousChangeLayer = \"https://maps.trpa.org/server/rest/services/Impervious_Surface_Change_2010_to_2019/MapServer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.analysis.Identity(\n",
    "    in_features=r\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.Impervious\\SDE.Impervious_2019\",\n",
    "    identity_features=r\"F:\\GIS\\DB_CONNECT\\Vector.sde\\SDE.EIP\\SDE.Existing_Drainage_Areas\",\n",
    "    out_feature_class=r\"C:\\GIS\\Scratch.gdb\\Impervious_2019_Identity\",\n",
    "    join_attributes=\"ALL\",\n",
    "    cluster_tolerance=None,\n",
    "    relationship=\"NO_RELATIONSHIPS\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arcpy.management.CalculateGeometryAttributes(\n",
    "    in_features=\"Impervious_2019_Identity\",\n",
    "    geometry_property=\"Acres AREA\",\n",
    "    length_unit=\"\",\n",
    "    area_unit=\"ACRES_US\",\n",
    "    coordinate_system='PROJCS[\"NAD_1983_UTM_Zone_10N\",GEOGCS[\"GCS_North_American_1983\",DATUM[\"D_North_American_1983\",SPHEROID[\"GRS_1980\",6378137.0,298.257222101]],PRIMEM[\"Greenwich\",0.0],UNIT[\"Degree\",0.0174532925199433]],PROJECTION[\"Transverse_Mercator\"],PARAMETER[\"False_Easting\",500000.0],PARAMETER[\"False_Northing\",0.0],PARAMETER[\"Central_Meridian\",-123.0],PARAMETER[\"Scale_Factor\",0.9996],PARAMETER[\"Latitude_Of_Origin\",0.0],UNIT[\"Meter\",1.0]]',\n",
    "    coordinate_format=\"SAME_AS_INPUT\"\n",
    ")\n",
    "\n",
    "arcpy.conversion.ExportTable(\n",
    "    in_table=\"Impervious_2019_Identity\",\n",
    "    out_table=r\"F:\\GIS\\DB_CONNECT\\Tabular.sde\\SDE.ClimateResilience_Impervious_Identify_Areawide_ByYear\",\n",
    "    where_clause=\"\",\n",
    "    use_field_alias_as_name=\"NOT_USE_ALIAS\",\n",
    "    field_mapping='FID_Impervious_2019 \"FID_Impervious_2019\" true true false 4 Long 0 0,First,#,Impervious_2019_Identity,FID_Impervious_2019,-1,-1;Feature \"Feature\" true true false 8 Text 0 0,First,#,Impervious_2019_Identity,Feature,0,7;Surface \"Surface\" true true false 4 Text 0 0,First,#,Impervious_2019_Identity,Surface,0,3;FID_Existing_Drainage_Areas \"FID_Existing_Drainage_Areas\" true true false 4 Long 0 0,First,#,Impervious_2019_Identity,FID_Existing_Drainage_Areas,-1,-1;Drainage_Area_Name \"Name\" true true false 100 Text 0 0,First,#,Impervious_2019_Identity,Drainage_Area_Name,0,99;Status \"Status\" true true false 50 Text 0 0,First,#,Impervious_2019_Identity,Status,0,49;Year_Completed \"Year_Completed\" true true false 4 Text 0 0,First,#,Impervious_2019_Identity,Year_Completed,0,3;Acres \"Acres\" true true false 8 Double 0 0,First,#,Impervious_2019_Identity,Acres,-1,-1',\n",
    "    sort_field=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# REST service for impervious area wide\n",
    "imperviousAreaWide = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/140\"\n",
    "\n",
    "# Create a FeatureLayer object\n",
    "feature_layer = FeatureLayer(imperviousAreaWide)\n",
    "# Query the feature layer and convert the result to a Spatially Enabled DataFrame\n",
    "sdf_impervious = feature_layer.query().sdf\n",
    "\n",
    "# summarize total area of Surface = Hard Surface\n",
    "sdf_impervious_hard = sdf_impervious[sdf_impervious['Surface'] == 'Hard']\n",
    "\n",
    "# summarize the area of hard surface covered by status = completed or active\n",
    "sdf_impervious_hard_summary = sdf_impervious_hard.groupby(['Status', 'Year_Completed'])['Acres'].sum().reset_index()\n",
    "\n",
    "# filter out Status is not ''\n",
    "sdf_impervious_hard_summary = sdf_impervious_hard_summary[sdf_impervious_hard_summary['Status'] != '']\n",
    "# group status active and constructed to completed\n",
    "sdf_impervious_hard_summary['Status'] = sdf_impervious_hard_summary['Status'].replace(['Active', 'Constructed'], 'Completed')\n",
    "\n",
    "# add acres in cumulative sum\n",
    "sdf_impervious_hard_summary['Cumulative Acres'] = sdf_impervious_hard_summary.groupby('Status')['Acres'].cumsum()\n",
    "\n",
    "total_acres = sdf_impervious_hard['Acres'].sum()\n",
    "\n",
    "# create cumulative sum of acres of status - completed\n",
    "sdf_impervious_hard_summary['Cumulative Acres'] = sdf_impervious_hard_summary['Acres'].cumsum()\n",
    "\n",
    "# subtract area covereed by cumulatve sum from total acres\n",
    "sdf_impervious_hard_summary['Acres Remaining'] = total_acres - sdf_impervious_hard_summary['Cumulative Acres']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf_impervious_hard_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# areawide overlay URL\n",
    "areawideOverlay = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/140\"\n",
    "# get data from map service\n",
    "data = get_fs_data(areawideOverlay)\n",
    "# summarize total area of Surface = Hard Surface\n",
    "df = data.loc[data['Surface'] == 'Hard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.Surface.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets data from the TRPA server\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    # return data frame\n",
    "    return all_data\n",
    "\n",
    "def get_areawide_data():\n",
    "    # areawide overlay URL\n",
    "    areawideOverlay = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/140\"\n",
    "    # get data from map service\n",
    "    data = get_fs_data(areawideOverlay)\n",
    "    # summarize total area of Surface = Hard Surface\n",
    "    df = data.loc[data['Surface'] == 'Hard']\n",
    "    # calculate total acres\n",
    "    total_acres = df['Acres'].sum()\n",
    "    # summarize the area of hard surface covered by status = completed or active\n",
    "    sdf_impervious_hard_summary = df.groupby(['Status', 'Year_Completed'])['Acres'].sum().reset_index()\n",
    "    # filter out Status is not ''\n",
    "    sdf_impervious_hard_summary = sdf_impervious_hard_summary[sdf_impervious_hard_summary['Status'] != '']\n",
    "    # group status active and constructed to completed\n",
    "    sdf_impervious_hard_summary['Status'] = sdf_impervious_hard_summary['Status'].replace(['Active', 'Constructed'], 'Completed')\n",
    "    # add acres in cumulative sum\n",
    "    sdf_impervious_hard_summary['Cumulative Acres'] = sdf_impervious_hard_summary.groupby('Status')['Acres'].cumsum()\n",
    "    # create cumulative sum of acres of status - completed\n",
    "    sdf_impervious_hard_summary['Cumulative Acres'] = sdf_impervious_hard_summary['Acres'].cumsum()\n",
    "    # subtract area covereed by cumulatve sum from total acres\n",
    "    sdf_impervious_hard_summary['Acres Remaining'] = total_acres - sdf_impervious_hard_summary['Cumulative Acres']\n",
    "    df = sdf_impervious_hard_summary\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_areawide_data()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# areawide overlay URL\n",
    "areawideOverlay = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/140\"\n",
    "# get data from map service\n",
    "data = get_fs_data(areawideOverlay)\n",
    "# summarize total area of Surface = Hard Surface\n",
    "df = data.loc[data['Surface'] == 'Hard']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate total acres\n",
    "total_acres = df['Acres'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_areawide_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ploty figure of impervious area wide by year\n",
    "fig = px.bar(sdf_impervious_hard_summary, x='Year_Completed', y='Cumulative Acres', color='Status', title='Impervious Area Wide by Year')\n",
    "fig.update_xaxes(title_text='Year')\n",
    "fig.update_yaxes(title_text='Acres of Hard Surface Covered by Yera')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ploty figure of impervious area wide by year\n",
    "fig = px.bar(sdf_impervious_hard_summary, x='Year_Completed', y=['Cumulative Acres', 'Acres Remaining'], title='Impervious Area Wide by Year')\n",
    "fig.update_xaxes(title_text='Year')\n",
    "fig.update_yaxes(title_text='Acres of Hard Surface Covered by Yera')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total number of housing units in town centers and share of affordable housing in Town Centers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arcgis.features import FeatureLayer\n",
    "import pandas as pd\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parcel development history layer for 2012, and 2018-2022 # 2021 and 2023 coming soon. other years are unknown\n",
    "parcelURL = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/17\"\n",
    "# deed restricted housing layer\n",
    "deedURL = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/20\"\n",
    "# query only the rows where yar is 2022\n",
    "parcelDevelopment2022 = FeatureLayer(parcelURL).query(where=\"Year = 2022\").sdf\n",
    "deedAffordableHousing = FeatureLayer(deedURL).query(\"DeedRestrictionType = 'Affordable Housing'\").sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge the two dataframes\n",
    "df = pd.merge(parcelDevelopment2022, deedAffordableHousing, how='left', on='APN')\n",
    "\n",
    "# group by LOCATION_TO_TOWNCENTER and sum of OBJECTID_y and Residential_Units\n",
    "dfTC = df.groupby('LOCATION_TO_TOWNCENTER').agg({'OBJECTID_y':'count', 'Residential_Units':'sum'}).reset_index()\n",
    "\n",
    "# add the values in the first row to the second row\n",
    "dfTC.iloc[1] = dfTC.iloc[0] + dfTC.iloc[1]\n",
    "\n",
    "# drop index row 0\n",
    "dfTC = dfTC.drop(dfTC.index[0])\n",
    "\n",
    "# rename column OBJECTID_y to Total Deed Restricted Housing\n",
    "dfTC = dfTC.rename(columns={'LOCATION_TO_TOWNCENTER':'Location to Town Center',\n",
    "                            'OBJECTID_y':'Total Deed Restricted Housing', \n",
    "                            'Residential_Units':'Total Residential Units'})\n",
    "\n",
    "# cast Total Deed Restricted Housing to int and Residential Units to int\n",
    "dfTC['Total Deed Restricted Housing'] = dfTC['Total Deed Restricted Housing'].astype(int)\n",
    "dfTC['Total Residential Units'] = dfTC['Total Residential Units'].astype(int)\n",
    "\n",
    "dfTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create plotly stacked bar chart of Deed Restricted Housing and residential units by Location to Town Center\n",
    "fig = px.bar(dfTC, x='Location to Town Center', y=['Total Deed Restricted Housing', 'Total Residential Units'], title='Deed Restricted Housing and Residential Units by Location to Town Center', barmode='stack')\n",
    "fig.update_xaxes(title_text='Location to Town Center')\n",
    "fig.update_yaxes(title_text='Total Units')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change in share of homes with electric or solar energy fuel compared to oil/gas over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kathleen did this. need to get the data in our database and web service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number of deed-restricted affordable, moderate, and achievable units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# deed restriction service\n",
    "deedRestrictionService = \"https://www.laketahoeinfo.org/WebServices/GetDeedRestrictedParcels/JSON/e17aeb86-85e3-4260-83fd-a2b32501c476\"\n",
    "\n",
    "# read in deed restricted parcels\n",
    "dfDeed = pd.read_json(deedRestrictionService)\n",
    "\n",
    "# filter out deed restrictions that are not affordable housing\n",
    "dfDeed = dfDeed.loc[dfDeed['DeedRestrictionType'].isin(['Affordable Housing', 'Achievable Housing', 'Moderate Income Housing'])]\n",
    "\n",
    "# create year column\n",
    "dfDeed['Year'] = dfDeed['RecordingDate'].str[-4:]\n",
    "\n",
    "# group by type and year\n",
    "df = dfDeed.groupby(['DeedRestrictionType', 'Year']).size().reset_index(name='Total')\n",
    "\n",
    "# sort by year\n",
    "df.sort_values('Year', inplace=True)\n",
    "\n",
    "# rename columns\n",
    "df = df.rename(columns={'DeedRestrictionType': 'Type', 'Year': 'Year', 'Total': 'Count'})\n",
    "\n",
    "# Create a DataFrame with all possible combinations of 'Type' and 'Year'\n",
    "df_all = pd.DataFrame({\n",
    "    'Type': np.repeat(df['Type'].unique(), df['Year'].nunique()),\n",
    "    'Year': df['Year'].unique().tolist() * df['Type'].nunique()\n",
    "})\n",
    "\n",
    "# Merge the new DataFrame with the original one to fill in the gaps of years for each type with NaN values\n",
    "df = pd.merge(df_all, df, on=['Type', 'Year'], how='left')\n",
    "\n",
    "# Replace NaN values in 'Count' with 0\n",
    "df['Count'] = df['Count'].fillna(0)\n",
    "\n",
    "# Ensure 'Count' is of integer type\n",
    "df['Count'] = df['Count'].astype(int)\n",
    "\n",
    "# Recalculate 'Cumulative Count' as the cumulative sum of 'Count' within each 'Type' and 'Year'\n",
    "df['Cumulative Count'] = df.sort_values('Year').groupby('Type')['Count'].cumsum()\n",
    "\n",
    "# create cumuluative total of deed restricted parcels by type\n",
    "fig = px.line(df, x=\"Year\", y=\"Cumulative Count\", color=\"Type\", title=\"Deed Restricted Parcels\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent of renewable energy as a share of total energy used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kathleen did this. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Total transit ridership by transit systems\n",
    "* https://www.laketahoeinfo.org/Indicator/Detail/46/Overview\n",
    "* this doesn't work yet. Talking to ESA to get web service stood up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from arcgis.features import FeatureLayer\n",
    "\n",
    "# Data not avaialble from LTinfo yet. Shannon is working on a service for this. I will get the data from Kira as a placeholder for now.\n",
    "# indicator data saved from Kira's email\n",
    "eipTransit = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/131\"\n",
    "\n",
    "# read in spatial enabled dataframe\n",
    "dfTransit = FeatureLayer(eipTransit).query().sdf\n",
    "\n",
    "# drop ObjectID\n",
    "dfTransit = dfTransit.drop(columns=['OBJECTID'])\n",
    "# stack data by month\n",
    "dfTransit = dfTransit.melt(id_vars=['MONTH'], var_name='Name', value_name='Ridership')\n",
    "\n",
    "# create Year field from last two characters of month but add 20 prefix\n",
    "dfTransit['Year'] = '20' + dfTransit['MONTH'].str[-2:]\n",
    "# strip the last three characters from month\n",
    "dfTransit['Month'] = dfTransit['MONTH'].str[:-3]\n",
    "# drop MONTH\n",
    "dfTransit = dfTransit.drop(columns=['MONTH'])\n",
    "# make the values in Month the real names of the months\n",
    "dfTransit['Month'] = dfTransit['Month'].replace(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n",
    "                                                ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "\n",
    "\n",
    "# create a Date type field of Month and Year\n",
    "dfTransit['Date'] = dfTransit['Month'] + ' ' + dfTransit['Year']\n",
    "# convert Date to datetime\n",
    "dfTransit['Date'] = pd.to_datetime(dfTransit['Date'], format='%B %Y')\n",
    "dfTransit = dfTransit.sort_values('Date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gets data from the TRPA server\n",
    "def get_fs_data(service_url):\n",
    "    feature_layer = FeatureLayer(service_url)\n",
    "    query_result = feature_layer.query()\n",
    "    # Convert the query result to a list of dictionaries\n",
    "    feature_list = query_result.features\n",
    "    # Create a pandas DataFrame from the list of dictionaries\n",
    "    all_data = pd.DataFrame([feature.attributes for feature in feature_list])\n",
    "    # return data frame\n",
    "    return all_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_transit():\n",
    "    url = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/131\"\n",
    "    # get data from map service\n",
    "    data = get_fs_data(url)\n",
    "    # drop ObjectID\n",
    "    data = data.drop(columns=['OBJECTID'])\n",
    "    # stack data by month\n",
    "    data = data.melt(id_vars=['MONTH'], var_name='Name', value_name='Ridership')\n",
    "\n",
    "    # create Year field from last two characters of month but add 20 prefix\n",
    "    data['Year'] = '20' + data['MONTH'].str[-2:]\n",
    "    # strip the last three characters from month\n",
    "    data['Month'] = data['MONTH'].str[:-3]\n",
    "    # drop MONTH\n",
    "    data = data.drop(columns=['MONTH'])\n",
    "    # make the values in Month the real names of the months\n",
    "    data['Month'] = data['Month'].replace(['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'], \n",
    "                                        ['January', 'February', 'March', 'April', 'May', 'June', 'July', 'August', 'September', 'October', 'November', 'December'])\n",
    "    # create a Date type field of Month and Year\n",
    "    data['Date'] = data['Month'] + ' ' + data['Year']\n",
    "    # convert Date to datetime\n",
    "    data['Date'] = pd.to_datetime(data['Date'], format='%B %Y')\n",
    "    df = data.sort_values('Date')\n",
    "    # drop Name = Total\n",
    "    df = df.loc[df['Name'] != 'Total']\n",
    "    # drop Name IN _Paratransit\n",
    "    df = df.loc[~df['Name'].str.contains('_Paratransit')]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data_transit()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a plotly figure of transit ridership by date \n",
    "fig = px.line(df, x=\"Date\", y=\"Ridership\", color=\"Name\", title=\"Transit Ridership\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily per capita Vehicles Miles Traveled (VMT) and progress towards VMT target. The RTP (2021) identifies a Daily per capita VMT target set at a 6.8 percent reduction from 2018 levels by 2045 (2018 per capita daily VMT is 12.48, goal is 11.63)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new data from Josh coming 2/6/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coverage of electric bus routes and alternative fuel, such as EV charging for vehicles and bicycles\n",
    "* Primary source: https://www.plugshare.com/ requested access to their API\n",
    "* Secondary (if needed to cross-check): https://afdc.energy.gov/stations/#/find/nearest\n",
    "\n",
    "* current in-house data: https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# no real info on this right now. Will checking with Kira on this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline mode share and weekday or seasonal variation. The Tahoe RTP (2021) includes the following Non-Auto Mode Share Target: Improve average non-auto mode share calculated from the two most recent TRPA travel survey results; the current performance on target at 24.5% (2018-20 average) up from 18% in 2014-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new data from Josh coming 2/6/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transportation access in priority communities. The Tahoe RTP (2021) includes a target to increase access to each mode for Priority communities to 100% by 2014."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will check in with Kira on this again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Increased lane miles of low-stress bicycle facilities (both bicycle and pedestrian facilities that are considered comfortable enough for all users and abilities, and implicitly measures active transportation network connectivity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from arcgis.features import FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the service URL\n",
    "bikelaneService = \"https://maps.trpa.org/server/rest/services/Transportation/MapServer/3\"\n",
    "\n",
    "# Create a FeatureLayer object\n",
    "feature_layer = FeatureLayer(bikelaneService)\n",
    "\n",
    "# Query the feature layer and convert the result to a Spatially Enabled DataFrame\n",
    "sdf_bikelane = feature_layer.query().sdf\n",
    "\n",
    "# recalc miles field from shape length\n",
    "sdf_bikelane.MILES = sdf_bikelane[\"Shape.STLength()\"]/ 1609.34\n",
    "\n",
    "# filter for CLASS = 1 2 or 3\n",
    "filtered_sdf_bikelane = sdf_bikelane[sdf_bikelane['CLASS'].isin(['1', '2', '3'])]\n",
    "\n",
    "# fix bad values\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['before 2010', ' before 2010'], '2010')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['before 2006','Before 2006','BEFORE 2006'], '2006')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace([' 2014'], '2014')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['2007 (1A) 2008 (1B)'], '2008')\n",
    "\n",
    "# drop rows with <NA> values\n",
    "filtered_sdf_bikelane = filtered_sdf_bikelane.dropna(subset=['YR_OF_CONS'])\n",
    "# drop rows with 'i dont know' or 'UNKNOWN' values\n",
    "filtered_sdf_bikelane = filtered_sdf_bikelane[~filtered_sdf_bikelane['YR_OF_CONS'].isin(['i dont know', 'UNKNOWN'])]\n",
    "\n",
    "# rename columns\n",
    "df = filtered_sdf_bikelane.rename(columns={'CLASS': 'Class', 'YR_OF_CONS': 'Year', 'MILES': 'Miles'})\n",
    "\n",
    "\n",
    "# Create a DataFrame with all possible combinations of 'Type' and 'Year'\n",
    "df_all = pd.DataFrame({\n",
    "    'Class': np.repeat(df['Class'].unique(), df['Year'].nunique()),\n",
    "    'Year': df['Year'].unique().tolist() * df['Class'].nunique()\n",
    "})\n",
    "\n",
    "# Merge the new DataFrame with the original one to fill in the gaps of years for each type with NaN values\n",
    "df = pd.merge(df_all, df, on=['Class', 'Year'], how='left')\n",
    "\n",
    "# add 2005 to the Year field for Class 1 2, and 3\n",
    "dict = {'Class':['1', '2', '3'], \n",
    "        'Year':['2005', '2005', '2005'], \n",
    "        'Miles':[0, 0, 0] \n",
    "       } \n",
    "  \n",
    "df2 = pd.DataFrame(dict) \n",
    "  \n",
    "df = pd.concat([df, df2], ignore_index = True) \n",
    "# cast Year as integer\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "# sort by year and miles\n",
    "df.sort_values(['Year', 'Miles'], inplace=True)\n",
    "\n",
    "# Replace NaN values in 'MILES' with 0\n",
    "df['Miles'] = df['Miles'].fillna(0)\n",
    "\n",
    "# Recalculate 'Cumulative Count' as the cumulative sum of 'Count' within each 'Type' and 'Year'\n",
    "df['Cumulative Count'] = df.sort_values('Year').groupby(['Year', 'Class'])['Miles'].cumsum()\n",
    "\n",
    "# get rid of all columns except for Year, Class, and Cumulative Count\n",
    "df = df[['Year', 'Class', 'Cumulative Count']]\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get rid of all columns except for Year, Class, and Cumulative Count\n",
    "df = df[['Year', 'Class', 'Cumulative Miles']]\n",
    "# get all rows where Year = 2006\n",
    "df_2006 = df[df['Year'] == 2006]\n",
    "# display all rows\n",
    "pd.set_option('display.max_rows', None)\n",
    "df_2006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(years)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from arcgis.features import FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the service URL\n",
    "bikelaneService = \"https://maps.trpa.org/server/rest/services/Transportation/MapServer/3\"\n",
    "\n",
    "# Create a FeatureLayer object\n",
    "feature_layer = FeatureLayer(bikelaneService)\n",
    "\n",
    "# Query the feature layer and convert the result to a Spatially Enabled DataFrame\n",
    "sdf_bikelane = feature_layer.query().sdf\n",
    "\n",
    "# recalc miles field from shape length\n",
    "sdf_bikelane.MILES = sdf_bikelane[\"Shape.STLength()\"]/ 1609.34\n",
    "\n",
    "# filter for CLASS = 1 2 or 3\n",
    "filtered_sdf_bikelane = sdf_bikelane[sdf_bikelane['CLASS'].isin(['1', '2', '3'])]\n",
    "\n",
    "# fix bad values\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['before 2010', ' before 2010'], '2010')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['before 2006','Before 2006','BEFORE 2006'], '2006')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace([' 2014'], '2014')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['2007 (1A) 2008 (1B)'], '2008')\n",
    "\n",
    "# drop rows with <NA> values\n",
    "filtered_sdf_bikelane = filtered_sdf_bikelane.dropna(subset=['YR_OF_CONS'])\n",
    "# drop rows with 'i dont know' or 'UNKNOWN' values\n",
    "filtered_sdf_bikelane = filtered_sdf_bikelane[~filtered_sdf_bikelane['YR_OF_CONS'].isin(['i dont know', 'UNKNOWN'])]\n",
    "\n",
    "# rename columns\n",
    "df = filtered_sdf_bikelane.rename(columns={'CLASS': 'Class', 'YR_OF_CONS': 'Year', 'MILES': 'Miles'})\n",
    "\n",
    "# Create a DataFrame with all possible combinations of 'Type' and 'Year'\n",
    "df_all = pd.DataFrame({\n",
    "    'Class': np.repeat(df['Class'].unique(), df['Year'].nunique()),\n",
    "    'Year': df['Year'].unique().tolist() * df['Class'].nunique()\n",
    "})\n",
    "\n",
    "# Merge the new DataFrame with the original one to fill in the gaps of years for each type with NaN values\n",
    "df = pd.merge(df_all, df, on=['Class', 'Year'], how='left')\n",
    "\n",
    "# add 2005 to the Year field for Class 1 2, and 3\n",
    "dict = {'Class':['1', '2', '3'], \n",
    "        'Year':['2005', '2005', '2005'], \n",
    "        'Miles':[0, 0, 0] \n",
    "       } \n",
    "  \n",
    "df2 = pd.DataFrame(dict) \n",
    "\n",
    "# bring in 2005 data  \n",
    "df = pd.concat([df, df2], ignore_index = True) \n",
    "\n",
    "# cast Year as integer\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# sort by year and miles\n",
    "df.sort_values(['Year', 'Miles'], inplace=True)\n",
    "\n",
    "# Replace NaN values in 'MILES' with 0\n",
    "df['Miles'] = df['Miles'].fillna(0)\n",
    "\n",
    "# \n",
    "df =df.groupby(['Year', 'Class'])['Miles'].sum().reset_index()\n",
    "\n",
    "\n",
    "df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "from arcgis.features import FeatureLayer\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "\n",
    "# Define the service URL\n",
    "bikelaneService = \"https://maps.trpa.org/server/rest/services/Transportation/MapServer/3\"\n",
    "\n",
    "# Create a FeatureLayer object\n",
    "feature_layer = FeatureLayer(bikelaneService)\n",
    "\n",
    "# Query the feature layer and convert the result to a Spatially Enabled DataFrame\n",
    "sdf_bikelane = feature_layer.query().sdf\n",
    "\n",
    "# recalc miles field from shape length\n",
    "sdf_bikelane.MILES = sdf_bikelane[\"Shape.STLength()\"]/ 1609.34\n",
    "\n",
    "# filter for CLASS = 1 2 or 3\n",
    "filtered_sdf_bikelane = sdf_bikelane[sdf_bikelane['CLASS'].isin(['1', '2', '3'])]\n",
    "\n",
    "# fix bad values\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['before 2010', ' before 2010'], '2010')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['before 2006','Before 2006','BEFORE 2006'], '2006')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace([' 2014'], '2014')\n",
    "filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'] = filtered_sdf_bikelane.loc[:, 'YR_OF_CONS'].replace(['2007 (1A) 2008 (1B)'], '2008')\n",
    "\n",
    "# drop rows with <NA> values\n",
    "filtered_sdf_bikelane = filtered_sdf_bikelane.dropna(subset=['YR_OF_CONS'])\n",
    "# drop rows with 'i dont know' or 'UNKNOWN' values\n",
    "filtered_sdf_bikelane = filtered_sdf_bikelane[~filtered_sdf_bikelane['YR_OF_CONS'].isin(['i dont know', 'UNKNOWN'])]\n",
    "\n",
    "# rename columns\n",
    "df = filtered_sdf_bikelane.rename(columns={'CLASS': 'Class', 'YR_OF_CONS': 'Year', 'MILES': 'Miles'})\n",
    "\n",
    "# Create a DataFrame with all possible combinations of 'Type' and 'Year'\n",
    "df_all = pd.DataFrame({\n",
    "    'Class': np.repeat(df['Class'].unique(), df['Year'].nunique()),\n",
    "    'Year': df['Year'].unique().tolist() * df['Class'].nunique()\n",
    "})\n",
    "\n",
    "# Merge the new DataFrame with the original one to fill in the gaps of years for each type with NaN values\n",
    "df = pd.merge(df_all, df, on=['Class', 'Year'], how='left')\n",
    "\n",
    "# add 2005 to the Year field for Class 1 2, and 3\n",
    "dict = {'Class':['1', '2', '3'], \n",
    "        'Year':['2005', '2005', '2005'], \n",
    "        'Miles':[0, 0, 0] \n",
    "       } \n",
    "  \n",
    "df2 = pd.DataFrame(dict) \n",
    "\n",
    "# bring in 2005 data  \n",
    "df = pd.concat([df, df2], ignore_index = True) \n",
    "\n",
    "# cast Year as integer\n",
    "df['Year'] = df['Year'].astype(int)\n",
    "\n",
    "# sort by year and miles\n",
    "df.sort_values(['Year', 'Miles'], inplace=True)\n",
    "\n",
    "# Replace NaN values in 'MILES' with 0\n",
    "df['Miles'] = df['Miles'].fillna(0)\n",
    "\n",
    "# create grouped dataframe\n",
    "df = df.groupby(['Year', 'Class'])['Miles'].sum().reset_index()\n",
    "\n",
    "# Recalculate 'Cumulative Count' as the cumulative sum of 'Count' within each 'Type' and 'Year'\n",
    "df['Cumulative Miles'] = df.sort_values('Year').groupby('Class')['Miles'].cumsum()\n",
    "\n",
    "# create cumuluative total of miles of bike lanes by Class\n",
    "fig = px.line(df, x=\"Year\", y=\"Cumulative Miles\", color=\"Class\", title=\"Miles of Bike Lane by Type\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Median Household Income (MHI) by community area (to be determined) and disaggregated by remote and non-remote workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census table\n",
    "censusTable = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/128\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing costs (median home sales price and rental rates, by jurisdiction); include cost per unit and cost per square foot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from social_systems import *\n",
    "\n",
    "\n",
    "def get_data_median_home_price():\n",
    "    url=\"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/147\"\n",
    "    \n",
    "    \n",
    "    data = get_fs_data(url)\n",
    "    # convert month and year to datetime\n",
    "    data[\"month_year\"] = pd.to_datetime(data[\"month_year\"])\n",
    "    data[\"year\"] = data[\"month_year\"].dt.year\n",
    "    data[\"month\"] = data[\"month_year\"].dt.month\n",
    "    # rename columns\n",
    "    df = data.rename(columns={\"Purchase_Amt\": \"Purchase Amount\", \"month_year\": \"Month\",\"year\":\"Year\"})\n",
    "    return df\n",
    "\n",
    "def plot_median_home_price(df):\n",
    "    trendline(\n",
    "        df,\n",
    "        path_html=\"html/4.1.b_Median_Sale_Prices.html\",\n",
    "        div_id=\"4.1.b_Median_Sale_Prices\",\n",
    "        x=\"Month\",\n",
    "        y=\"Purchase Amount\",\n",
    "        color=None,\n",
    "        color_sequence=[\"#208385\"],\n",
    "        orders=None,\n",
    "        sort=['Year',\"month\"],\n",
    "        x_title=\"Sale Date\",\n",
    "        y_title=\"Median Sale Price ($)\",\n",
    "        format=\",.0f\",\n",
    "        hovertemplate=\"%{y:,.0f}\",\n",
    "        markers=True,\n",
    "        hover_data=None,\n",
    "        tickvals=None,\n",
    "        ticktext=None,\n",
    "        tickangle=None,\n",
    "        hovermode=\"x unified\",\n",
    "        custom_data=None,\n",
    "        additional_formatting=None\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data_median_home_price()\n",
    "plot_median_home_price(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Housing tenure (rented full-time, owner-occupied, second home), disaggregated by race, ethnicity, and age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census data\n",
    "censusTable = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/128\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Percent of workers who commute into the basin, origin demographics, distance travelled, difference in travel time by mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from social_systems import *\n",
    "df = get_data_commute_patterns()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_fs_data\n",
    "data = get_fs_data(\n",
    "    \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/141\"\n",
    ")\n",
    "grouped_df = data.groupby([\"Year\", \"category\"], as_index=False).agg({\"S000\": \"sum\"})\n",
    "processed_df = grouped_df.pivot(index=\"Year\", columns=\"category\", values=\"S000\").reset_index()\n",
    "processed_df[\"commuter_percentage\"] = (\n",
    "    processed_df[\"Live elsewhere, work in Tahoe\"]\n",
    "    / (\n",
    "        processed_df[\"Live elsewhere, work in Tahoe\"]\n",
    "        + processed_df[\"Live in Tahoe, work in Tahoe\"]\n",
    "    )\n",
    "    * 100\n",
    ")\n",
    "processed_df.info()\n",
    "\n",
    "# group b\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import get_fs_data\n",
    "\n",
    "data = get_fs_data(\n",
    "    \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/141\"\n",
    ")\n",
    "\n",
    "#Helper function that is used to concatenate data return\n",
    "def create_or_append_df(df, summary_df):\n",
    "    if df.empty:\n",
    "        df = summary_df.copy()\n",
    "    else:\n",
    "        df = pd.concat([df, summary_df])\n",
    "    return df\n",
    "\n",
    "# blank dataframe\n",
    "df = pd.DataFrame()\n",
    "\n",
    "# dictionary of wage categories\n",
    "wage_categories = {\n",
    "    \"SE01\": \"Number of jobs with earnings of $1,250 a month or less\",\n",
    "    \"SE02\": \"Number of jobs with earnings of $1,251 to $3,333 a month\",\n",
    "    \"SE03\": \"Number of jobs with earnings of $3,334 a month or more\",\n",
    "}\n",
    "\n",
    "wages = ['SE01', 'SE02', 'SE03']\n",
    "\n",
    "# commuters by year broken down by wage classes\n",
    "for wage in wages:\n",
    "    # annual number of commutes in each of the job earnings classes\n",
    "    grouped_df = data.groupby([\"Year\", \"category\"], as_index=False).agg({wage: \"sum\"})\n",
    "    processed_df = grouped_df.pivot(index=\"Year\", columns=\"category\", values=wage).reset_index()\n",
    "    processed_df[\"commuter_percentage\"] = (\n",
    "        processed_df[\"Live elsewhere, work in Tahoe\"]\n",
    "        / (\n",
    "            processed_df[\"Live elsewhere, work in Tahoe\"]\n",
    "            + processed_df[\"Live in Tahoe, work in Tahoe\"]\n",
    "        )\n",
    "        * 100\n",
    "    )\n",
    "    processed_df[\"Wage Category\"] = wage\n",
    "    processed_df[\"Year\"] = processed_df[\"Year\"].astype(int)\n",
    "    \n",
    "    df = create_or_append_df(df, processed_df)\n",
    "\n",
    "df[\"Wage Category\"] = df[\"Wage Category\"].map(wage_categories)\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"AnnualCommuterType_by_WageCategory_LODES.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grouped_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transient Occupancy Tax revenue and changes over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## new data from Josh coming 2/6/2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Consistent employment and median wages by sector and overall "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Access to recreation sites, fresh food, and healthcare for zero-vehicle households"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking wiht Kira on this. Will make a map of what we have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Firewise communities in the Tahoe basin, coolling centers/heating centers, resources, and emergency infrastructure (medical centers with supplies, fire response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map made."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Population disaggregated by race and ethnicity, age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census data\n",
    "censusTable = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/128\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Number/share of households with access and functional needs (These can be referred to as vulnerable populations including populations such as persons with disabilities, older adults, children, limited English proficiency, and transportation disadvantages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# census data\n",
    "censusTable = \"https://maps.trpa.org/server/rest/services/LTinfo_Climate_Resilience_Dashboard/MapServer/128\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "trpa-311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
